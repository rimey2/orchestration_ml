{"cells": [{"cell_type": "markdown", "metadata": {"type": "intro"}, "source": ["Maintenant que nous avons plus de connaissances sur les donn\u00e9es, nous pouvons commencer par transformer les donn\u00e9es et entra\u00eener un premier mod\u00e8le. Notre choix se portera sur **LightGBM**, par sa rapidit\u00e9 d'entra\u00eenement et ses performances aussi proches que XGBoost.\n", "\n", "<blockquote><p>\ud83d\ude4b <b>Ce que nous allons faire</b></p>\n", "<ul>\n", "    <li>Transformer le jeu de donn\u00e9es pour construire la base d'apprentissage</li>\n", "    <li>Calibrer un mod\u00e8le LightGBM</li>\n", "    <li>\u00c9valuer les r\u00e9sultats</li>\n", "</ul>\n", "</blockquote>\n", "\n", "<img src=\"https://media.giphy.com/media/xUA7b4arnbo3THfzi0/giphy.gif\" width=\"300\" />"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Transformation des donn\u00e9es\n", "\n", "Reprenons l'\u00e9chantillon que nous avons \u00e9tudi\u00e9 auparavant."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "baaefdd5d05e2728"}, "outputs": [], "source": ["import os\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "sns.set()\n", "\n", "# La fonction expanduser permet de convertir le r\u00e9pertoire ~ en chemin absolu\n", "data = pd.read_csv(os.path.expanduser(\"~/data/sample.csv\"))\n", "print(\"Taille de l'\u00e9chantillon :\", data.shape[0])\n", "print(\"Taille m\u00e9moire : {:2.2f} Mb\".format(data.memory_usage().sum() / 1e6))\n", "data.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Comme \u00e9voqu\u00e9 pr\u00e9c\u00e9demment, nous cherchons \u00e0 conna\u00eetre si, lors d'une session, un utilisateur va acheter un produit ou non. Ainsi, notre base d'apprentissage va \u00eatre constitu\u00e9 de sessions dont on sait si l'utilisateur a achet\u00e9 ce produit ou non.\n", "\n", "Chaque observation de notre base d'apprentissage disposera des informations suivantes.\n", "\n", "- Les informations li\u00e9es au produit (nombre de vues du produit dans la session, cat\u00e9gorie, identifiant du produit).\n", "- Les informations li\u00e9es \u00e0 la session (dur\u00e9e en secondes, nombres de sessions pr\u00e9c\u00e9dentes, heure de d\u00e9but).\n", "\n", "Nous allons donc **transformer l'\u00e9chantillon** afin d'obtenir ces informations."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Informations sur les produits\n", "\n", "Conform\u00e9ment \u00e0 notre objectif, notre base d'apprentissage repr\u00e9sentera un ensemble de sessions o\u00f9, pour chaque session et chaque produit, l'utilisateur a achet\u00e9 (colonne `purchased` \u00e0 $1$) ce produit ou non (colonne `purchased` \u00e0 $0$). Il faudra donc **agr\u00e9ger les lignes** ayant la m\u00eame session utilisateur et pour le m\u00eame produit.\n", "\n", "Commen\u00e7ons par construire la colonne `purchased`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "4f6a958426a5d4e1"}, "outputs": [], "source": ["# On r\u00e9cup\u00e8re les \u00e9v\u00e9nements dont la colonne user_session n'est pas nulle\n", "events_per_session = data.loc[~data[\"user_session\"].isna(), :].copy()\n", "\n", "# On cr\u00e9e une colonne purchase qui vaut 0 ou 1 en fonction de la colonne event_type\n", "events_per_session['purchased'] = np.where(events_per_session['event_type'] == \"purchase\", 1, 0)\n", "# On agr\u00e8ge par session et par produit pour savoir si ce dernier a \u00e9t\u00e9 achet\u00e9 dans la session\n", "events_per_session['purchased'] = events_per_session \\\n", "    .groupby([\"user_session\", \"product_id\"])['purchased'] \\\n", "    .transform(\"max\")\n", "\n", "events_per_session.sample(n=5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Si la colonne `purchased` vaut $1$, cela signifie que durant la session, l'utilisateur a achet\u00e9 le produit concern\u00e9.\n", "\n", "Effectuons la m\u00eame chose pour le nombre de vues dans la session en cr\u00e9ant la colonne `num_views`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "e0e77baad16b6fc0"}, "outputs": [], "source": ["# On d\u00e9termine combien de fois l'utilisateur a vu de produits dans la session\n", "events_per_session['num_views_session'] = np.where(events_per_session['event_type'] == \"view\", 1, 0)\n", "events_per_session['num_views_session'] = events_per_session.groupby([\"user_session\"]) \\\n", "    ['num_views_session'].transform(\"sum\")\n", "\n", "# On d\u00e9termine combien de fois l'utilisateur a vu un produit en particulier dans la session\n", "events_per_session['num_views_product'] = np.where(events_per_session['event_type'] == \"view\", 1, 0)\n", "events_per_session['num_views_product'] = events_per_session.groupby([\"user_session\", \"product_id\"]) \\\n", "    ['num_views_product'].transform(\"sum\")\n", "\n", "events_per_session.sample(n=5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Nous allons \u00e9galement effectuer un `split` sur la colonne `category_code` pour r\u00e9cup\u00e9rer la cat\u00e9gorie et la sous-cat\u00e9gorie de chaque produit (si elles existent)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "d157ae8d79b15e17"}, "outputs": [], "source": ["events_per_session['category'] = events_per_session['category_code'] \\\n", "    .str.split(\".\",expand=True)[0].astype('category')\n", "events_per_session['sub_category'] = events_per_session['category_code'] \\\n", "    .str.split(\".\",expand=True)[1].astype('category')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Enfin, nous convertissons la colonne `event_time` en objet `datetime`, ce qui nous permet de r\u00e9cup\u00e9rer pr\u00e9cis\u00e9ment l'heure, la minute et le jour de la semaine associ\u00e9s \u00e0 l'\u00e9v\u00e9nement."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "9d24de1e36361a40"}, "outputs": [], "source": ["events_per_session[\"event_time\"] = pd.to_datetime(events_per_session[\"event_time\"], utc=True)\n", "\n", "events_per_session[\"hour\"] = events_per_session[\"event_time\"].dt.hour\n", "events_per_session[\"minute\"] = events_per_session[\"event_time\"].dt.minute\n", "events_per_session[\"weekday\"] = events_per_session[\"event_time\"].dt.dayofweek"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Informations sur les sessions\n", "\n", "Regardons maintenant les sessions de chaque utilisateur. Nous allons commencer par d\u00e9terminer les dur\u00e9es de chaque session. Pour cela, la m\u00e9thode la plus facile consiste \u00e0 regrouper les observations par `user_session`, puis \u00e0 calculer le minimum et le maximum de la colonne `event_time`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "b629a4bceb993977"}, "outputs": [], "source": ["# On calcule la date minimum et maximum de chaque session\n", "sessions_duration = events_per_session \\\n", "    .groupby(\"user_session\").agg(\n", "        {\"event_time\": [np.min, np.max]}\n", "    ) \\\n", "    .reset_index()\n", "\n", "sessions_duration[\"amax\"] = pd.to_datetime(sessions_duration[\"event_time\"][\"amax\"])\n", "sessions_duration[\"amin\"] = pd.to_datetime(sessions_duration[\"event_time\"][\"amin\"])\n", "del sessions_duration[\"event_time\"]\n", "# On aplati au niveau 0 les colonnes\n", "sessions_duration.columns = sessions_duration.columns.get_level_values(0)\n", "# On calcule la dur\u00e9e totale, en secondes, de chaque TimeDelta\n", "sessions_duration[\"duration\"] = (sessions_duration[\"amax\"] - sessions_duration[\"amin\"]).dt.seconds\n", "sessions_duration.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Une fois calcul\u00e9, nous avons simplement besoin de faire **une jointure** avec le DataFrame `events_per_session`, dont le r\u00e9sultat sera stock\u00e9 dans le DataFrame `dataset`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "b1bda23b91ea83ff"}, "outputs": [], "source": ["dataset = events_per_session \\\n", "    .sort_values(\"event_time\") \\\n", "    .drop_duplicates([\"event_type\", \"product_id\", \"user_id\", \"user_session\"]) \\\n", "    .loc[events_per_session[\"event_type\"].isin([\"cart\", \"purchase\"])] \\\n", "    .merge(\n", "        sessions_duration[[\"user_session\", \"duration\"]],\n", "        how=\"left\",\n", "        on=\"user_session\"\n", "    )\n", "\n", "dataset.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notons qu'ici nous utilisons un `drop_duplicates` pour supprimer les lignes doublons : elles correspondent au m\u00eame produit, type d'\u00e9v\u00e9nement, utilisateur et session (par exemple, un utilisateur qui visite deux fois le m\u00eame produit dans une seule session).\n", "\n", "<div class=\"alert alert-block alert-info\">  \n", "    Nous avons d\u00e9j\u00e0 r\u00e9colt\u00e9 cette informations plus haut en calculant le nombre de vues par session.\n", "</div>\n", "\n", "Autre question qui peut se r\u00e9v\u00e9ler int\u00e9ressante : l'utilisateur a-t-il d\u00e9j\u00e0 eu d'autres sessions auparavant ? Pour cela, il est important d'\u00e9tablir une **relation d'ordre**, notamment avec `event_time`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "f8fee4c35d58a6b3"}, "outputs": [], "source": ["count_prev_sessions = dataset[[\"user_id\", \"user_session\", \"event_time\"]] \\\n", "    .sort_values(\"event_time\") \\\n", "    .groupby([\"user_id\", \"user_session\"]) \\\n", "    .first() \\\n", "    .reset_index()\n", "\n", "# cumcount permet de faire un comptage cumul\u00e9 des lignes : 0, 1, 2, 3, ...\n", "count_prev_sessions[\"num_prev_sessions\"] = count_prev_sessions \\\n", "    .sort_values(\"event_time\") \\\n", "    .groupby(\"user_id\") \\\n", "    .cumcount()\n", "\n", "count_prev_sessions.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Comme toujours, nous effectuons une jointure avec `dataset`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "1467bf51eabe33f0"}, "outputs": [], "source": ["dataset = dataset.merge(\n", "    count_prev_sessions[[\"user_session\", \"num_prev_sessions\"]],\n", "    how=\"left\",\n", "    on=\"user_session\"\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Une derni\u00e8re information qui pourrait \u00e9galement \u00eatre utile, c'est de savoir si l'utilisateur a d\u00e9j\u00e0 vu un produit en particulier dans une session pr\u00e9c\u00e9dente. Elle ressemble fortement \u00e0 la transformation pr\u00e9c\u00e9dente, si ce n'est qu'il faut \u00e9galement prendre en compte le `product_id` et le `user_id`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "956fa2d587a3bd39"}, "outputs": [], "source": ["view_prev_session = dataset[[\"user_id\", \"user_session\", \"event_time\", \"product_id\"]]\n", "view_prev_session = view_prev_session \\\n", "    .sort_values(\"event_time\") \\\n", "    .groupby([\"user_id\", \"user_session\", \"product_id\"]) \\\n", "    .first() \\\n", "    .reset_index()\n", "\n", "view_prev_session[\"num_prev_product_views\"] = view_prev_session \\\n", "    .sort_values(\"event_time\") \\\n", "    .groupby([\"user_id\", \"product_id\"]) \\\n", "    .cumcount()\n", "\n", "dataset = dataset.merge(\n", "    view_prev_session[[\"user_session\", \"product_id\", \"num_prev_product_views\"]],\n", "    how=\"left\",\n", "    on=[\"user_session\", \"product_id\"]\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Enfin, nous allons supprimer les lignes doublons par rapport aux colonnes `user_session`, `product_id` et `purchased`, si par exemple un utilisateur a achet\u00e9 plus d'une fois un m\u00eame produit (auquel cas, l'issue serait toujours la classe positive \u00e0 partir du moment o\u00f9 le produit est achet\u00e9 au moins une fois)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "1e7ecb2f6f681d2d"}, "outputs": [], "source": ["dataset = dataset \\\n", "    .sort_values(\"event_time\") \\\n", "    .drop_duplicates([\"user_session\", \"product_id\", \"purchased\"]) \\\n", "    .drop([\"event_time\", \"event_type\", \"category_code\", \"category_id\"], axis=1)\n", "\n", "dataset.to_csv(os.path.expanduser(\"~/data/dataset.csv\"), index=False)\n", "dataset.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Nous pouvons maintenant v\u00e9rifier la proportion des deux classes pr\u00e9sentes dans le jeu de donn\u00e9es."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "48708e6d65632d51"}, "outputs": [], "source": ["print(\"Taille du jeu de donn\u00e9es :\", dataset.shape[0])\n", "print(\"Proportion des classes :\")\n", "dataset[\"purchased\"].value_counts() / dataset.shape[0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notre jeu de donn\u00e9es n'est d\u00e9s\u00e9quilibr\u00e9, il n'y a donc pas de r\u00e9-\u00e9chantillonnage \u00e0 appliquer."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "3e2baeef928ccd19"}, "outputs": [], "source": ["dataset.to_csv(os.path.expanduser(\"~/data/primary.csv\"), index=False)"]}, {"cell_type": "markdown", "metadata": {"break": "new"}, "source": ["## Feature Engineering\n", "\n", "Maintenant que notre DataFrame `dataset` contient les observations, nous devons appliquer une \u00e9tape de **feature engineering** pour que la base d'apprentissage soit transform\u00e9e num\u00e9riquement et utilisable par notre mod\u00e8le. Il n'y a, en r\u00e9alit\u00e9, que trois colonnes \u00e0 encoder : `brand`, `category` et `sub_category`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "e918f8503aca5f61"}, "outputs": [], "source": ["from sklearn.preprocessing import LabelEncoder\n", "\n", "# Le DataFrame features est la base d'apprentissage encod\u00e9e num\u00e9riquement\n", "features = dataset.drop([\"user_id\", \"user_session\"], axis=1).copy()\n", "\n", "for label in [\"category\", \"sub_category\", \"brand\"]:\n", "    features[label] = features[label].astype(str)\n", "    features.loc[features[label] == 'nan', label] = \"unknown\"\n", "    features.loc[:, label] = LabelEncoder().fit_transform(features.loc[:, label].copy())\n", "\n", "features['weekday'] = features['weekday'].astype(int)\n", "features.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["V\u00e9rifions qu'il n'y a pas de valeurs manquantes."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "35858c03cc8f0855"}, "outputs": [], "source": ["features.isna().sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u00c0 partir de l\u00e0, nous allons cr\u00e9er la base d'apprentissage $(X, y)$ constitu\u00e9e des *features* puis s\u00e9parer la base en deux sous-ensembles."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "dc95fcd8c42b772b"}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X = features.drop(\"purchased\", axis=1)\n", "y = features['purchased']\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(\n", "    X, y, test_size=0.25, random_state=40\n", ")\n", "\n", "X_train.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Nous allons ensuite enregistrer les donn\u00e9es g\u00e9n\u00e9r\u00e9es.\n", "\n", "<div class=\"alert alert-block alert-warning\">\n", "    Il est important d'avoir une consistance sur les ensembles utilis\u00e9s pour entra\u00eener les mod\u00e8les et pour les \u00e9valuer.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "afed946bbdf820c5"}, "outputs": [], "source": ["X.to_csv(os.path.expanduser(\"~/data/X.csv\"), index=False)\n", "y.to_csv(os.path.expanduser(\"~/data/y.csv\"), index=False)\n", "\n", "X_train.to_csv(os.path.expanduser(\"~/data/X_train.csv\"), index=False)\n", "X_test.to_csv(os.path.expanduser(\"~/data/X_test.csv\"), index=False)\n", "y_train.to_csv(os.path.expanduser(\"~/data/y_train.csv\"), index=False)\n", "y_test.to_csv(os.path.expanduser(\"~/data/y_test.csv\"), index=False)"]}, {"cell_type": "markdown", "metadata": {"break": "new"}, "source": ["## Mod\u00e9lisation avec LightGBM\n", "\n", "L'algorithme que nous utiliserons est **LightGBM** : il a l'avantage d'\u00eatre plus rapide que XGBoost tout en conservant des performances proches.\n", "\n", "### Cr\u00e9ation des arbres\n", "\n", "Bien que similaire, la principale diff\u00e9rence entre LightGBM et XGBoost est la mani\u00e8re dont les arbres sont construits.\n", "\n", "XGBoost applique une m\u00e9thode appel\u00e9e **level-wise tree growth**, o\u00f9 chaque arbre est construit niveau par niveau. Ainsi, XGBoost d\u00e9veloppe le premier niveau (profondeur 1), puis d\u00e9veloppe le second niveau (profondeur 2) d\u00e8s lors que le niveau pr\u00e9c\u00e9dent a \u00e9t\u00e9 \u00e9tendu. Il s'agit d'une approche horizontale.\n", "\n", "LightGBM applique une m\u00e9thode appel\u00e9e **leaf-wise tree growth**. Contrairement \u00e0 la m\u00e9thode pr\u00e9c\u00e9dente, l'\u00e9lagage n'est pas r\u00e9alis\u00e9 en fonction de la profondeur, mais directement en fonction des feuilles. Ainsi, LightGBM \u00e9tendra chaque feuille de mani\u00e8re verticale, sans s'assurer que la profondeur concern\u00e9e est totalement \u00e9tendue. Cela peut produire, par exemple, des arbres qui ne sont pas \u00e9quilibr\u00e9s (d'o\u00f9 la possibilit\u00e9 de toujours ajouter l'hyper-param\u00e8tre `max_depth`).\n", "\n", "<div class=\"alert alert-block alert-info\">  \n", "    Si on ne faisait pas d'\u00e9lagage d'arbres, les arbres construits avec XGBoost et LightGBM seraient identiques.\n", "</div>\n", "\n", "Une des principales raisons pour laquelle LightGBM est tr\u00e8s souvent plus rapide que XGBoost est due \u00e0 l'utilisation de la m\u00e9thode *leaf-wise*.\n", "\n", "LightGBM permet \u00e9galement d'utiliser le **Gradient-based One-Side Sampling** (GOSS), qui consiste \u00e0 s\u00e9lectionner majoritairement des observations don les gradients sont \u00e9lev\u00e9s. Par exemple, si sur 10k lignes, seules 1k lignes pr\u00e9sentent des gradients \u00e9lev\u00e9s, LightGBM s\u00e9lectionnerai ces 1k observations, auxquelles il y ajoutera un certain nombres d'observations parmi les 9k restantes.\n", "\n", "Les hyper-param\u00e8tres les plus importants pour LightGBM sont les suivants.\n", "\n", "- `num_leaves` qui contr\u00f4le le nombre de feuilles (noeuds) dans chaque arbre.\n", "- `min_child_samples` sp\u00e9cifie le nombre d'observations dans une feuille. Il s'agit d'un hyper-param\u00e8tre importante pour \u00e9viter un sur-apprentissage, notamment lorsque le nombre de feuilles est \u00e9lev\u00e9.\n", "- `max_depth` pour contr\u00f4ler la profondeur maximale de chaque arbre.\n", "- `n_estimators` pour d\u00e9terminer le nombre d'arbres \u00e0 calibrer.\n", "- `learning_rate` pour sp\u00e9cifier le taux d'apprentissage de chaque arbre."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "4246edc3af25fcfa"}, "outputs": [], "source": ["from lightgbm.sklearn import LGBMClassifier\n", "from sklearn.model_selection import RepeatedKFold\n", "from sklearn.metrics import f1_score"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Entra\u00eenons un classifieur LightGBM avec les param\u00e8tres par d\u00e9faut. Nous allons mettre en place un <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold\" target=\"_blank\">Repeated k-Fold</a> pour calculer des scores sur plusieurs instances du mod\u00e8le.\n", "\n", "<div class=\"alert alert-block alert-info\">  \n", "    Pour rappel, la validation crois\u00e9e est importante ici pour s'assurer que le score calcul\u00e9 pour un mod\u00e8le est coh\u00e9rent.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "2e0ae1b649920974"}, "outputs": [], "source": ["# On r\u00e9p\u00e8te 3 fois un 5-Fold\n", "rep_kfold = RepeatedKFold(n_splits=4, n_repeats=3)\n", "\n", "# Hyper-param\u00e8tres des mod\u00e8les\n", "hyp_params = {\n", "    \"num_leaves\": 60,\n", "    \"min_child_samples\": 10,\n", "    \"max_depth\": 12,\n", "    \"n_estimators\": 100,\n", "    \"learning_rate\": 0.1\n", "}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Nous laisserons les param\u00e8tres par d\u00e9faut de l'objet `LGBMClassifier`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "f4e5b3c427989e0f"}, "outputs": [], "source": ["scores_train = []\n", "scores_test = []\n", "n_iter = 1\n", "for train_I, test_I in rep_kfold.split(X):\n", "    print(\"It\u00e9ration {} du k-Fold\".format(n_iter))\n", "    # On r\u00e9cup\u00e8re les indices des sous-\u00e9chantillons\n", "    X_fold_train = X.iloc[train_I, :]\n", "    y_fold_train = y.iloc[train_I]\n", "    X_fold_test = X.iloc[test_I, :]\n", "    y_fold_test = y.iloc[test_I]\n", "    \n", "    # On entra\u00eene un LightGBM avec les param\u00e8tres par d\u00e9faut\n", "    model = LGBMClassifier(**hyp_params, objective=\"binary\", verbose=-1)\n", "    model.fit(X_fold_train, y_fold_train)\n", "\n", "    # On calcule le score du mod\u00e8le sur le test\n", "    scores_train.append(\n", "        f1_score(y_fold_train, model.predict(X_fold_train))\n", "    )\n", "    scores_test.append(\n", "        f1_score(y_fold_test, model.predict(X_fold_test))\n", "    )\n", "    n_iter += 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Regardons maintenant les scores sur les deux sous-ensembles."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "03570ff5db447640"}, "outputs": [], "source": ["import matplotlib.ticker as mtick\n", "\n", "print(\"Score Train m\u00e9dian : {:2.1f}%\".format(np.mean(scores_train) * 100))\n", "print(\"Score Test m\u00e9dian : {:2.1f}%\".format(np.mean(scores_test) * 100))\n", "\n", "scores = pd.DataFrame.from_dict({\n", "    \"Train Set\": scores_train,\n", "    \"Test Set\": scores_test\n", "})\n", "\n", "plt.figure(figsize=(14, 4))\n", "sns.boxplot(data=scores, orient=\"h\")\n", "plt.title(\"Scores des mod\u00e8les du k-Fold\", fontsize=17)\n", "plt.gca().xaxis.set_major_formatter(mtick.PercentFormatter(1, 1))\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Les scores m\u00e9dians pour les pour les ensembles d'entra\u00eenement et de test sont respectivement $91.9\\%$ et $87.9\\%$. Tout d'abord, nous remarquons d'apr\u00e8s le graphique que les variations de scores sont faibles (un \u00e9cart-type d'environ $0.5\\%$ pour l'ensemble de test). Ensuite, la diff\u00e9rence entre les deux scores m\u00e9dian est d'environ $4\\%$, ce qui montre qu'il n'y a pas de sur-apprentissage apparent de nos mod\u00e8les.\n", "\n", "\u00c0 partir des ces r\u00e9sultats, nous pouvons maintenant calibrer un LightGBM sur le couple `(X_train, y_train)`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "b0ac0363c48c2341"}, "outputs": [], "source": ["model = LGBMClassifier(**hyp_params, objective=\"binary\", verbose=-1)\n", "model.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Calculons \u00e9galement les scores sur les ensembles."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "06471b264c09cd35"}, "outputs": [], "source": ["print(\"Score Train : {:2.1f}%\".format(f1_score(y_train, model.predict(X_train)) * 100))\n", "print(\"Score Test : {:2.1f}%\".format(f1_score(y_test, model.predict(X_test)) * 100))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Comme pr\u00e9vu, les scores sont tr\u00e8s similaires \u00e0 ceux obtenus lors du $k$-Fold."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \u00c9valuation du mod\u00e8le\n", "\n", "L'\u00e9valuation du mod\u00e8le est importante, car elle va nous permettre de s'assurer que le mod\u00e8le est suffisamment performant, tout en ayant un comportement pr\u00e9visible.\n", "\n", "<div class=\"alert alert-block alert-warning\">\n", "    Les scores ne doivent pas \u00eatre les seules m\u00e9triques pour \u00e9valuer un mod\u00e8le de Machine Learning.\n", "</div>\n", "\n", "Outre les scores, le mod\u00e8le doit \u00e9galement satisfaire d'autres contraintes. En effet, dans notre cas d'application orient\u00e9 marketing, nous souhaitons savoir si un client va finaliser un achat lors de son parcours sur la plateforme ECommerce.\n", "\n", "L'objectif ici est **d'inciter l'utilisateur \u00e0 finaliser son panier**. Pour cela, une m\u00e9thode consiste \u00e0 proposer des r\u00e9ductions sur-mesure pour augmenter la probabilit\u00e9 que l'utilisateur termine par un achat. Il s'agit donc d'une probl\u00e9matique de classification binaire. Nous pouvons ainsi faire deux types d'erreurs.\n", "\n", "- Un **faux positif** consiste \u00e0 pr\u00e9dire que l'utilisateur va acheter le produit alors qu'en r\u00e9alit\u00e9, il ne finalisera pas l'achat. Dans ce cas, proposer une r\u00e9duction n'a que peu d'impact, puisqu'il y a de forte chances que l'utilisateur ne l'utilise pas.\n", "- Un **faux n\u00e9gatif** consiste \u00e0 pr\u00e9dire que l'utilisateur ne va pas acheter le produit alors qu'en r\u00e9alit\u00e9, il ne finalisera l'achat. Dans ce deuxi\u00e8me cas de figure, donner une r\u00e9duction \u00e0 cet utilisateur implique une perte de b\u00e9n\u00e9fice, puisque l'utilisateur aurait tout de m\u00eame achet\u00e9 le produit, et ce m\u00eame sans code de r\u00e9duction.\n", "\n", "Ici, nous voyons que le faux n\u00e9gatif a **plus d'impact** en terme \u00e9conomique que le faux positif. En effet, il vaut mieux qu'un utilisateur ach\u00e8te \u00e0 prix r\u00e9duit plut\u00f4t qu'il n'ach\u00e8te pas un produit. \u00c0 l'inverse, on a tout int\u00e9r\u00eat \u00e0 ne pas proposer de r\u00e9duction \u00e0 un utilisateur qui ach\u00e8terai un produit m\u00eame sans code de r\u00e9duction.\n", "\n", "Notre objectif, c'est donc de d\u00e9tecter correctement les faux n\u00e9gatifs, puisque ces derniers impactent les b\u00e9n\u00e9fices, contraitement au faux positif.\n", "\n", "> \u2753 Comment \u00e9valuer si notre mod\u00e8le a plus tendance \u00e0 produire des faux positifs ou des faux n\u00e9gatifs ?\n", "\n", "Pour cela, il faut bien comprendre qu'en classification binaire, la sortie du mod\u00e8le n'est pas une classe ($0$ ou $1$), mais **une probabilit\u00e9** (d'\u00eatre dans chacune des deux classes).\n", "\n", "Il existe implicitement un seuil $\\alpha \\in [0, 1]$ \u00e0 partir duquel on consid\u00e8re que si $\\mathbb{P}(y = 1 | X=x) \\geq \\alpha$ alors la classe pr\u00e9dite est positive. Bien \u00e9videmment, en fonction de ce seuil $\\alpha$, la part d'observations en faux positif ou faux n\u00e9gatif ne seront pas identiques.\n", "\n", "Regardons quelques statistiques qui se basent sur la **matrice de confusion**.\n", "\n", "- La **sp\u00e9cificit\u00e9** repr\u00e9sente la probabilit\u00e9 de pr\u00e9dire un non achat parmi les utilisateurs n'ayant pas achet\u00e9.\n", "\n", "$$\\frac{TN}{FP+TN}$$\n", "\n", "- La **sensibilit\u00e9** (ou **rappel**) repr\u00e9sente la probabilit\u00e9 de pr\u00e9dire un achat parmi les utilisateurs ayant achet\u00e9.\n", "\n", "$$\\frac{TP}{TP+FN}$$\n", "\n", "- La **pr\u00e9cision** repr\u00e9sente la probabilit\u00e9 d'avoir des utilisateurs ayant achet\u00e9 parmi les utilisateurs dont un achat a \u00e9t\u00e9 pr\u00e9dit.\n", "\n", "$$\\frac{TP}{TP+FP}$$\n", "\n", "Les deux premi\u00e8res mesures s'int\u00e9ressent aux taux **parmi la connaissance th\u00e9orique** du comportement, alors que la derni\u00e8re s'int\u00e9resse au taux **parmi les pr\u00e9dictions du mod\u00e8le**.\n", "\n", "Nous avons dit qu'il est important d'avoir le moins de faux n\u00e9gatif : c'est donc **la sensibilit\u00e9 (ou rappel)** qui doit \u00eatre observ\u00e9e. Un rappel faible signifie que la part de faux n\u00e9gatifs est importante, alors qu'un rappel fort signifie que la part de faux n\u00e9gatifs est proche de $0$.\n", "\n", "<div class=\"alert alert-block alert-warning\">\n", "    Le rappel, \u00e0 lui seul, ne peut pas d\u00e9terminer enti\u00e8rement si le mod\u00e8le est performant ! Un mod\u00e8le peut tr\u00e8s bien avoir un rappel tr\u00e8s \u00e9lev\u00e9, mais une pr\u00e9cision tr\u00e8s basse.\n", "</div>\n", "\n", "Cet exemple correspond \u00e0 un mod\u00e8le qui pr\u00e9dirait tout le temps $1$ : il n'y aurait aucun faux n\u00e9gatif (car il ne pr\u00e9dit jamais $0$), mais la pr\u00e9cision serait tr\u00e8s mauvaise car il y aurait beaucoup de faux positifs. C'est pour cela qu'on utilise \u00e9galement le **F1 score**, qui fait intervenir \u00e0 la fois la pr\u00e9cision et le rappel.\n", "\n", "$$\\text{Score}_{\\text{F1}}=2 \\frac{\\text{Precision} \\times \\text{Rappel}}{\\text{Precision} + \\text{Rappel}}$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "317a7fa911660deb"}, "outputs": [], "source": ["from sklearn.metrics import recall_score, precision_score\n", "\n", "print(\"Precision Test : {:2.1f}%\".format(precision_score(y_test, model.predict(X_test)) * 100))\n", "print(\"Recall Test : {:2.1f}%\".format(recall_score(y_test, model.predict(X_test)) * 100))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Nous avons un rappel tr\u00e8s \u00e9lev\u00e9, ce qui signifie qu'il y a peu de faux n\u00e9gatifs. En revanche, la pr\u00e9cision est moins bonne : il y plus de faux positifs, notre mod\u00e8le \u00e0 souvent tendance \u00e0 trop pr\u00e9dire un achat alors que ce ne sera pas le cas.\n", "\n", "> \u2753 Comment pouvons-nous am\u00e9liorer la pr\u00e9cision ?\n", "\n", "L'int\u00e9r\u00eat maintenant, c'est de d\u00e9terminer quels sont les hyper-param\u00e8tres optimaux qui vont nous permettre d'obtenir les meilleurs performances. Pour cela, une pratique courante est **l'AutoML**, qui consiste \u00e0 automatiser l'entra\u00eenement des mod\u00e8les.\n", "\n", "Et tr\u00e8s souvent, en essayant de d\u00e9terminer les meilleurs hyper-param\u00e8tres, plut\u00f4t que d'utiliser une recherche par grille, les **m\u00e9thodes bay\u00e9siennes** sont plus courantes."]}, {"cell_type": "markdown", "metadata": {"type": "ending"}, "source": ["## \u2714\ufe0f Conclusion\n", "\n", "Nous venons de mettre en place notre premier mod\u00e8le pr\u00e9dictif sur nos donn\u00e9es.\n", "\n", "- Le jeu de donn\u00e9es a \u00e9t\u00e9 transform\u00e9 pour \u00eatre envoy\u00e9 au mod\u00e8le pr\u00e9dictif.\n", "- Avec un k-Fold, nous avons entra\u00een\u00e9 un LightGBM.\n", "- Nous avons mesur\u00e9 les performances obtenues.\n", "\n", "> \u27a1\ufe0f Le r\u00f4le du ML Engineer est, \u00e0 terme, d'automatiser cette \u00e9tape qui consiste \u00e0 trouver le meilleur mod\u00e8le (AutoML). C'est ce que nous allons voir avec <b>l'optimisation bay\u00e9sienne</b>."]}], "metadata": {"celltoolbar": "\u00c9diter les M\u00e9ta-Donn\u00e9es", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}