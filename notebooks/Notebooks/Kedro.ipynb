{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "type": "intro"
   },
   "source": [
    "Il est temps d'organiser tout notre pipeline ML, qui est actuellement sÃ©parÃ© dans plusieurs Notebooks. Pour nous aider, nous allons utiliser **Kedro**, un outil open-source permettant de crÃ©er des projets de Machine Learning reproductibles, maintenables et modulaires (i.e. plusieurs fichiers), le tout sans trop d'efforts. C'est donc un outil sur-mesure pour les ML Engineers !\n",
    "\n",
    "<blockquote><p>ğŸ™‹ <b>Ce que nous allons faire</b></p>\n",
    "<ul>\n",
    "    <li>CrÃ©er un premier projet Kedro et comprendre son architecture</li>\n",
    "    <li>Comprendre les concepts de Kedro</li>\n",
    "    <li>Construire un premier pipeline de traitement de donnÃ©es</li>\n",
    "</ul>\n",
    "</blockquote>\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/ZVUu5Pm23hDZS/giphy.gif\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kedro\n",
    "\n",
    "Qu'est-ce que Kedro, et pourquoi avons-nous besoin de cet outil ? La mÃ©thode classique pour construire des modÃ¨les de Machine Learning est d'utiliser Jupyter Notebook. Mais cette mÃ©thode n'est pas du tout viable, notamment lorsqu'il s'agit de dÃ©ployer le modÃ¨le en production dans un futur proche. Face Ã  cette situation, on prÃ©fÃ¨re donc construire un projet entier, dont les Notebooks sont en rÃ©alitÃ© des phases de recherche, d'expÃ©rimentation mais ne constituent pas en soi le coeur de sujet du projet. DÃ¨s lors que l'on met en place une architecture de code source, il est nÃ©cessaire d'adopter de bonnes pratiques, aussi bien hÃ©ritÃ©es des environnements IT que celles utilisÃ©es par les Data Scientists.\n",
    "\n",
    "<img src=\"https://repository-images.githubusercontent.com/182067506/4c724a00-48f4-11ea-84a5-cf8292b07d8e\" width=\"600\">\n",
    "\n",
    "Kedro a Ã©tÃ© dÃ©veloppÃ© pour appliquer ces bonnes pratiques tout au long d'un projet de Machine Learning.\n",
    "\n",
    "- Ã‰viter au maximum de dÃ©pendre de Jupyter Notebooks qui empÃªche la production d'un code source maintenable et reproductible.\n",
    "- AmÃ©liorer la collaboration entre les diffÃ©rents acteurs (Data Scientists, Data Engineers, DevOps) aux compÃ©tences diverses dans un projet.\n",
    "- Augmenter l'efficacitÃ© en appliquant la modularitÃ© du code, les sÃ©parations entre les donnÃ©es et leur utilisation ou encore en optimisant les exÃ©cutions de traitements atomiques.\n",
    "\n",
    "En bref, Kedro nous permet d'avoir un projet Python **entiÃ¨rement pensÃ©** pour le Machine Learning et optimisÃ© dans ce sens. Il existe d'autres alternatives Ã  Kedro (comme Kubeflow qui se base sur Kubernetes), mais il a l'avantage d'Ãªtre rapide Ã  prendre en main et possÃ¨de une communautÃ© dÃ©jÃ  active.\n",
    "\n",
    "### Premiers pas avec Kedro\n",
    "\n",
    "Essayons de crÃ©er un premier projet avec Kedro que nous allons nommer `purchase-predict`.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    La version que nous utiliserons est 0.17.0 : en utilisant une version plus rÃ©cente, il se peut que des erreurs de compatibilitÃ© surviennent. Il est donc conseillÃ© d'utiliser la version 0.17.0 en local pour suivre le cours.\n",
    "</div>\n",
    "\n",
    "CrÃ©ons un nouveau terminal et un nouveau projet Kedro."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "kedro new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous est demandÃ© un nom de projet. Nous laissons ensuite les autres informations vides (la valeur par dÃ©faut est affichÃ©e entre crochets)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "console"
   },
   "source": [
    "Project Name:\n",
    "=============\n",
    "Please enter a human readable name for your new project.\n",
    "Spaces and punctuation are allowed.\n",
    " [New Kedro Project]: purchase-predict\n",
    "\n",
    "Repository Name:\n",
    "================\n",
    "Please enter a directory name for your new project repository.\n",
    "Alphanumeric characters, hyphens and underscores are allowed.\n",
    "Lowercase is recommended.\n",
    " [purchase-predict]: \n",
    "\n",
    "Python Package Name:\n",
    "====================\n",
    "Please enter a valid Python package name for your project package.\n",
    "Alphanumeric characters and underscores are allowed.\n",
    "Lowercase is recommended. Package name must start with a letter or underscore.\n",
    " [purchase_predict]: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec cette commande, Kedro gÃ©nÃ¨re le dossier `purchase-predict` et y configure une architecture par dÃ©faut."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "console"
   },
   "source": [
    "â”œâ”€â”€ conf\n",
    "â”‚Â Â  â”œâ”€â”€ base\n",
    "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ catalog.yml\n",
    "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ credentials.yml\n",
    "â”‚Â Â  â”‚Â Â  â”œâ”€â”€ logging.yml\n",
    "â”‚Â Â  â”‚Â Â  â””â”€â”€ parameters.yml\n",
    "â”‚Â Â  â”œâ”€â”€ local\n",
    "â”‚Â Â  â””â”€â”€ README.md\n",
    "â”œâ”€â”€ data\n",
    "â”‚Â Â  â”œâ”€â”€ 01_raw\n",
    "â”‚Â Â  â”œâ”€â”€ 02_intermediate\n",
    "â”‚Â Â  â”œâ”€â”€ 03_primary\n",
    "â”‚Â Â  â”œâ”€â”€ 04_feature\n",
    "â”‚Â Â  â”œâ”€â”€ 05_model_input\n",
    "â”‚Â Â  â”œâ”€â”€ 06_models\n",
    "â”‚Â Â  â”œâ”€â”€ 07_model_output\n",
    "â”‚Â Â  â””â”€â”€ 08_reporting\n",
    "â”œâ”€â”€ docs\n",
    "â”‚Â Â  â””â”€â”€ source\n",
    "â”‚Â Â      â”œâ”€â”€ conf.py\n",
    "â”‚Â Â      â””â”€â”€ index.rst\n",
    "â”œâ”€â”€ logs\n",
    "â”‚Â Â  â””â”€â”€ journals\n",
    "â”œâ”€â”€ notebooks\n",
    "â”œâ”€â”€ pyproject.toml\n",
    "â”œâ”€â”€ README.md\n",
    "â”œâ”€â”€ setup.cfg\n",
    "â””â”€â”€ src\n",
    "    â”œâ”€â”€ purchase_predict\n",
    "    â”‚Â Â  â”œâ”€â”€ cli.py\n",
    "    â”‚Â Â  â”œâ”€â”€ hooks.py\n",
    "    â”‚Â Â  â”œâ”€â”€ __init__.py\n",
    "    â”‚Â Â  â”œâ”€â”€ pipelines\n",
    "    â”‚Â Â  â”‚Â Â  â””â”€â”€ __init__.py\n",
    "    â”‚Â Â  â”œâ”€â”€ run.py\n",
    "    â”‚Â Â  â””â”€â”€ settings.py\n",
    "    â”œâ”€â”€ requirements.txt\n",
    "    â”œâ”€â”€ setup.py\n",
    "    â””â”€â”€ tests\n",
    "        â”œâ”€â”€ __init__.py\n",
    "        â”œâ”€â”€ pipelines\n",
    "        â”‚Â Â  â””â”€â”€ __init__.py\n",
    "        â””â”€â”€ test_run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DÃ©taillons tout d'abord chaque dossier de premier niveau.\n",
    "\n",
    "- `conf` contient tous les fichiers de configuration des paramÃ¨tres (code, modÃ¨le) ainsi que les clÃ©s et secrets nÃ©cessaires.\n",
    "- `data` contient plusieurs dossiers qui correspondent aux donnÃ©es utilisÃ©es ou produits Ã  chaque Ã©tape du pipeline (base d'apprentissage, matrices des *features*, modÃ¨le sÃ©rialisÃ©).\n",
    "- `docs` contient des fichiers de documentation.\n",
    "- `logs` contient les journaux d'Ã©vÃ©nements de Kedro.\n",
    "- `notebooks` permet de stocker des notebooks.\n",
    "- `src` contient tous les codes nÃ©cessaire pour crÃ©er les pipelines.\n",
    "\n",
    "C'est notamment dans le dossier `src` que nous dÃ©velopperons les briques Ã©lÃ©mentaires et que nous les connecterons ensembles afin de former les diffÃ©rents pipelines.\n",
    "\n",
    "Avant de rentrer dans les concepts de Kedro, il est **fortement conseillÃ©** (si ce n'est indispensable) de crÃ©er un environnement virtuel Ã  la racine du projet."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "python3 -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "break": "new"
   },
   "source": [
    "## Concepts de Kedro\n",
    "\n",
    "Kedro ne se contente pas uniquement de crÃ©er un projet : il va Ã©galement apporter des fonctionnalitÃ©s trÃ¨s puissantes qui en font sa popularitÃ©.\n",
    "\n",
    "### Data Catalog\n",
    "\n",
    "Pour utiliser des donnÃ©es tout au long d'un pipeline ML, il est dÃ©conseillÃ© d'inscrire des chemins d'accÃ¨s en dur dans le code. Il est prÃ©fÃ©rable d'attribuer des noms Ã  des donnÃ©es existantes que l'on utilise ou celles que l'on crÃ©e. C'est rÃ´le du Data Catalog : nous allons dÃ©finir en amont des rÃ©fÃ©rentiels de donnÃ©es avec des noms associÃ©s. Le Data Catalog est rÃ©fÃ©rencÃ© dans le fichier `conf/base/catalog.yml`.\n",
    "\n",
    "Par dÃ©faut, Kedro propose plusieurs sous-dossiers dans `data` qui permet de mieux organiser les donnÃ©es.\n",
    "\n",
    "- `raw`, `intermediate` et `primary` font rÃ©fÃ©rence aux donnÃ©es brutes, celles ayant subi des traitements intermÃ©diaires et celles prÃªtes Ã  Ãªtre encodÃ©es.\n",
    "- `feature` contiendrait la base d'apprentissage $(X,y)$ encodÃ©e.\n",
    "- `model_input` contiendrait les Ã©chantillons d'entraÃ®nement et de test fournis au modÃ¨le.\n",
    "- `models` contiendrait le ou les modÃ¨les sÃ©rialisÃ©s.\n",
    "- `model_output` et `reporting` contiendraient les sorties du modÃ¨les ainsi que les graphes pour valider et interpÃ©ter.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    â„¹ï¸ Bien entendu, nous ne sommes pas tenu de suivre exactement cette structure, il s'agit plutÃ´t d'une organisation par dÃ©faut proposÃ©e par Kedro.\n",
    "</div>\n",
    "\n",
    "Dans notre cas, nous n'allons pas effectuer la transformation des donnÃ©es avec Kedro, puisque ce sera une tÃ¢che Spark SQL qui en sera chargÃ©e. Ainsi, nous aurons uniquement le donnÃ©es transformÃ© qui subira ensuite l'encodage. Nous considÃ©rons donc que l'Ã©chantillon que recevra Kedro sera situÃ© dans `primary` et sera nommÃ© `primary.csv`.\n",
    "\n",
    "Ã€ partir de ces donnÃ©es `primary.csv`, nous encoderons vers un nouveau fichier `dataset.csv` dans le dossier `feature`, dont nous rÃ©cupÃ©rerons les sous-ensembles d'apprentissage et de test. Pour rÃ©fÃ©rencer tous ces fichiers dans le Data Catalog, nous Ã©ditons le fichier `conf/base/catalog.yml` en spÃ©cifiant le nom, le type de donnÃ©es et le chemin en relatif par rapport au dossier racine."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "download_as": "catalog.yml",
    "format": "yaml"
   },
   "source": [
    "# Here you can define all your data sets by using simple YAML syntax.\n",
    "#\n",
    "# Documentation for this file format can be found in \"The Data Catalog\"\n",
    "# Link: https://kedro.readthedocs.io/en/stable/05_data/01_data_catalog.html\n",
    "\n",
    "primary:\n",
    "  type: pandas.CSVDataSet\n",
    "  filepath: data/03_primary/primary.csv\n",
    "\n",
    "dataset:\n",
    "  type: pandas.CSVDataSet\n",
    "  filepath: data/04_feature/dataset.csv\n",
    "\n",
    "X_train:\n",
    "  type: pandas.CSVDataSet\n",
    "  filepath: data/05_model_input/X_train.csv\n",
    "\n",
    "y_train:\n",
    "  type: pandas.CSVDataSet\n",
    "  filepath: data/05_model_input/y_train.csv\n",
    "\n",
    "X_test:\n",
    "  type: pandas.CSVDataSet\n",
    "  filepath: data/05_model_input/X_test.csv\n",
    "\n",
    "y_test:\n",
    "  type: pandas.CSVDataSet\n",
    "  filepath: data/05_model_input/y_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'avantage du Data Catalog est la flexibilitÃ© d'utilisation. PlutÃ´t que de spÃ©cifier le chemin d'accÃ¨s dans chaque fichier Python, nous pouvons simplement inscrire `primary` comme argument, et Kedro va automatiquement charger en mÃ©moire (ici au format CSV avec `pandas`) ce jeu de donnÃ©es. Ainsi, nous pouvons Ã  tout moment modifier la valeur du chemin `filepath` ici sans altÃ©rer tous les fichiers Python.\n",
    "\n",
    "### Nodes et Pipelines\n",
    "\n",
    "Parmi les concepts les plus importants, nous retrouvons celui des **nodes** et des **pipelines**.\n",
    "\n",
    "Un **node** est un Ã©lÃ©ment unitaire qui reprÃ©sente une tÃ¢che. Par exemple, nous pouvons imaginer un node pour encoder le jeu de donnÃ©es, un autre pour construire les sous-ensembles d'entraÃ®nement et de test, et un dernier pour calibrer un modÃ¨le de Machine Learning.\n",
    "\n",
    "Un **pipeline**, Ã  l'instar des pipelines de donnÃ©es, est une succession de nodes qui peuvent Ãªtre assemblÃ©s en sÃ©quence ou en parallÃ¨le.\n",
    "\n",
    "Les pipelines sont une partie trÃ¨s importante dans Kedro. Reprenons le pipeline d'expÃ©rimentation oÃ¹ l'on entraÃ®ne un modÃ¨le de Machine Learning.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/kedro2.png\" />\n",
    "\n",
    "Ce qui est important Ã  voir, c'est le caractÃ¨re sÃ©quencÃ© entre les tÃ¢ches. En particulier, **impossible d'entraÃ®ner un modÃ¨le sans avoir encodÃ© les donnÃ©es**, tout comme il est impossible d'Ã©valuer un modÃ¨le si l'on n'en a pas.\n",
    "\n",
    "Kedro va nous permettre de crÃ©er ces pipelines, garantissant que les **artifacts** (donnÃ©es construites, modÃ¨les, etc) vont Ãªtre disponibles pour les autres nodes du pipeline. C'est un outil essentiel car il va nous assurer que les traitements sont homogÃ¨nes et que lorsque l'on souhaitera entraÃ®ner un nouveau modÃ¨le par exemple, les donnÃ©es subiront exactement le mÃªme traitement puisqu'elles passeront par le mÃªme pipeline. On Ã©vite ainsi les risques d'oubli ou d'erreur de cohÃ©rence entre deux exÃ©cutions successives, chose qui arrive plus souvent que l'on ne l'imagine avec les Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "break": "new"
   },
   "source": [
    "## Premier pipeline\n",
    "\n",
    "Nous allons crÃ©er ensemble un premier pipeline qui va contenir deux noeuds.\n",
    "\n",
    "- Un premier qui va se charger d'encoder le jeu de donnÃ©es `primary`.\n",
    "- Un autre qui va sÃ©parer la base de donnÃ©es en deux sous-ensembles d'entraÃ®nement et d'apprentissage.\n",
    "\n",
    "CommenÃ§ons tout d'abord par tÃ©lÃ©charger le fichier d'Ã©chantillon dans le dossier `data/03_primary`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "cp ~/data/primary.csv ~/purchase-predict/data/03_primary/primary.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrÃ©ons un dossier `processing` dans `src/purchase_predict/pipelines`. Nous allons y ajouter deux fichiers Python `nodes.py` et `pipeline.py`.\n",
    "\n",
    "- Le fichier `nodes.py` contient les dÃ©finitions des fonctions qui seront utilisÃ©es par les nodes.\n",
    "- Le fichier `pipeline.py` permet de construire le pipeline Ã  partir de nodes qui utiliseront les fonctions du fichier `nodes.py`.\n",
    "\n",
    "Puisque nous avons deux noeuds, nous devons construire deux fonctions.\n",
    "\n",
    "### Noeud `encode_features`\n",
    "\n",
    "La fonction `encode_features` va rÃ©cupÃ©rer `dataset`, qui correspond au fichier CSV ayant subit les transformations (notamment via Spark SQL)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def encode_features(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode features of data file.\n",
    "    \"\"\"\n",
    "    features = dataset.drop([\"user_id\", \"user_session\"], axis=1).copy()\n",
    "\n",
    "    encoders = []\n",
    "    for label in [\"category\", \"sub_category\", \"brand\"]:\n",
    "        features[label] = features[label].astype(str)\n",
    "        features.loc[features[label] == \"nan\", label] = \"unknown\"\n",
    "        encoder = LabelEncoder()\n",
    "        features.loc[:, label] = encoder.fit_transform(features.loc[:, label].copy())\n",
    "        encoders.append((label, encoder))\n",
    "\n",
    "    features[\"weekday\"] = features[\"weekday\"].astype(int)\n",
    "    return dict(features=features, transform_pipeline=encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction retourne le DataFrame `features`, qui correspond aux donnÃ©es encodÃ©es.\n",
    "\n",
    "### Noeud `split_dataset`\n",
    "\n",
    "L'autre fonction, `split_dataset`, opÃ¨re simplement une sÃ©paration en deux sous-Ã©chantillons. L'argument `test_ratio` permettra de spÃ©cifier la proportion d'observation Ã  considÃ©rer dans le sous-Ã©chantillon de test."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "def split_dataset(dataset: pd.DataFrame, test_ratio: float) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Splits dataset into a training set and a test set.\n",
    "    \"\"\"\n",
    "    X = dataset.drop(\"purchased\", axis=1)\n",
    "    y = dataset[\"purchased\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_ratio, random_state=40\n",
    "    )\n",
    "\n",
    "    return dict(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction retourne les quatre DataFrames.\n",
    "\n",
    "### Construction du pipeline\n",
    "\n",
    "Pour entamer la construction du pipeline, nous allons tout d'abord dÃ©finir un **paramÃ¨tre** Kedro, le `test_ratio`. En effet, il s'agit d'un paramÃ¨tre de configuration qui doit Ãªtre initialisÃ© au prÃ©alable, et plutÃ´t que d'inscrire en dur dans le code la valeur du ratio pour l'ensemble de test, tout comme le Data Catalog, le fichier `parameters.yml` dans le dossier `conf/base` permet de centraliser tous les paramÃ¨tres du modÃ¨le, Cloud, de traitement de donnÃ©es, etc."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "download_as": "parameters.yml",
    "format": "yaml"
   },
   "source": [
    "test_ratio: 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ã€ partir de lÃ , nous pouvons construire notre pipeline. Pour cela, nous utilisons l'objet `Pipeline` de Kedro, qui s'attends Ã  une liste de `node`. Chaque instance de `node` attends trois paramÃ¨tres.\n",
    "\n",
    "- Le nom de la fonction Python qui sera appelÃ©e.\n",
    "- Les arguments de la fonction (sous forme de liste ou dictionnaire).\n",
    "- Les sorties du modÃ¨les (sous forme de liste ou dictionnaire)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from kedro.pipeline import Pipeline, node\n",
    "\n",
    "from .nodes import encode_features, split_dataset\n",
    "\n",
    "def create_pipeline(**kwargs):\n",
    "    return Pipeline(\n",
    "        [\n",
    "            node(\n",
    "                encode_features,\n",
    "                \"primary\",\n",
    "                dict(features=\"dataset\", transform_pipeline=\"transform_pipeline\")\n",
    "            ),\n",
    "            node(\n",
    "                split_dataset,\n",
    "                [\"dataset\", \"params:test_ratio\"],\n",
    "                dict(\n",
    "                    X_train=\"X_train\",\n",
    "                    y_train=\"y_train\",\n",
    "                    X_test=\"X_test\",\n",
    "                    y_test=\"y_test\"\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le premier noeud appelle la fonction `encode_features` avec pour argument le jeu de donnÃ©es `primary`, et le rÃ©sultat (un seul) retournÃ© par la fonction sera stockÃ© dans le jeu de donnÃ©es `dataset`.\n",
    "\n",
    "Le deuxiÃ¨me noeud nÃ©cessite le jeu de donnÃ©es `dataset` ainsi que le paramÃ¨tre `test_ratio`, et retourne les 4 DataFrames qui correspond aux sous-ensembles.\n",
    "\n",
    "Notre pipeline est donc en place, et nous pouvons la visualiser ci-dessous.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/kedro1.jpg\" width=\"800\" />\n",
    "\n",
    "Avant de lancer le pipeline, installons les quelques dÃ©pendances nÃ©cessaires dans l'environnement virtuel (qui est vierge par dÃ©faut)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour terminer, il faut crÃ©er une instance du pipeline pour pouvoir l'exÃ©cuter. Toutes les instances sont dÃ©finies dans le fichier `hooks.py` Ã  la racine de `src/purchase_predict`. Ajoutons l'importation suivante."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from purchase_predict.pipelines.processing import pipeline as processing_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous importons donc le fichier `pipeline` dans `pipelines.processing` dont nous crÃ©ons l'alias `processing_pipeline`. En appelant la fonction `processing_pipeline.create_pipeline()`, cela va instancier un nouveau pipeline qui contient les deux noeuds.\n",
    "\n",
    "Ces instanciations doivent Ãªtre dÃ©finies dans la fonction `register_pipelines`, qui retourne un dictionnaire oÃ¹ chaque clÃ© est le nom du pipeline et la valeur l'objet pipeline associÃ©."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "@hook_impl\n",
    "def register_pipelines(self) -> Dict[str, Pipeline]:\n",
    "    \"\"\"Register the project's pipeline.\n",
    "\n",
    "    Returns:\n",
    "        A mapping from a pipeline name to a ``Pipeline`` object.\n",
    "\n",
    "    \"\"\"\n",
    "    p_processing = processing_pipeline.create_pipeline()\n",
    "\n",
    "    return {\"processing\": p_processing}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est notamment ici qu'il sera Ã©galement possible d'imbriquer sÃ©quentiellement plusieurs pipelines entre-eux.\n",
    "\n",
    "Notre pipeline est donc prÃªt, nous pouvons l'exÃ©cuter."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "kedro run --pipeline processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "console"
   },
   "source": [
    "2021-01-04 17:45:19,425 - kedro.framework.session.session - WARNING - Unable to git describe /home/jovyan/purchase_predict\n",
    "2021-01-04 17:45:19,463 - root - INFO - ** Kedro project purchase_predict\n",
    "2021-01-04 17:45:19,486 - kedro.io.data_catalog - INFO - Loading data from `primary` (CSVDataSet)...\n",
    "2021-01-04 17:45:19,565 - kedro.pipeline.node - INFO - Running node: encode_features([primary]) -> [dataset]\n",
    "2021-01-04 17:45:19,597 - kedro.io.data_catalog - INFO - Saving data to `dataset` (CSVDataSet)...\n",
    "2021-01-04 17:45:19,760 - kedro.runner.sequential_runner - INFO - Completed 1 out of 2 tasks\n",
    "2021-01-04 17:45:19,760 - kedro.io.data_catalog - INFO - Loading data from `dataset` (CSVDataSet)...\n",
    "2021-01-04 17:45:19,817 - kedro.io.data_catalog - INFO - Loading data from `params:test_ratio` (MemoryDataSet)...\n",
    "2021-01-04 17:45:19,817 - kedro.pipeline.node - INFO - Running node: split_dataset([dataset,params:test_ratio]) -> [X_test,X_train,y_test,y_train]\n",
    "2021-01-04 17:45:19,828 - kedro.io.data_catalog - INFO - Saving data to `X_train` (CSVDataSet)...\n",
    "2021-01-04 17:45:19,918 - kedro.io.data_catalog - INFO - Saving data to `y_train` (CSVDataSet)...\n",
    "2021-01-04 17:45:19,950 - kedro.io.data_catalog - INFO - Saving data to `X_test` (CSVDataSet)...\n",
    "2021-01-04 17:45:19,986 - kedro.io.data_catalog - INFO - Saving data to `y_test` (CSVDataSet)...\n",
    "2021-01-04 17:45:19,999 - kedro.runner.sequential_runner - INFO - Completed 2 out of 2 tasks\n",
    "2021-01-04 17:45:19,999 - kedro.runner.sequential_runner - INFO - Pipeline execution completed successfully.\n",
    "2021-01-04 17:45:19,999 - kedro.framework.session.store - INFO - `save()` not implemented for `BaseSessionStore`. Skipping the step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En dÃ©roulant le dossier `data/05_model_input`, nous devrions voir apparaÃ®tre les 4 fichiers CSV gÃ©nÃ©rÃ©s par le pipeline.\n",
    "\n",
    "### Visualisation des pipelines\n",
    "\n",
    "La derniÃ¨re dÃ©pendance `kedro-viz` peut Ãªtre utile car elle permet de visualiser les diffÃ©rents pipelines directement depuis le navigateur."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "pip install kedro-viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour lancer le serveur de visualisation, il suffit d'exÃ©cuter la commande suivante (attention de vÃ©rifier que l'environnement virtuel est bien activÃ©)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "kedro viz --port 4141"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois lancÃ©, nous pouvons y accÃ©der <a href=\"jupyter://user-redirect/proxy/4141/\" target=\"_blank\">par ce lien</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "type": "ending"
   },
   "source": [
    "## âœ”ï¸ Conclusion\n",
    "\n",
    "Tu viens de mettre en place ton premier pipeline avec Kedro !\n",
    "\n",
    "- Nous avons vu crÃ©Ã© notre premier projet avec Kedro.\n",
    "- Nous avons dÃ©taillÃ© les concepts importants que l'on rencontre avec Kedro.\n",
    "- Nous avons mis en place un premier pipeline.\n",
    "\n",
    "> â¡ï¸ AprÃ¨s avoir construit le pipeline de traitement de donnÃ©es, place au **pipeline d'entraÃ®nement du modÃ¨le**."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Ã‰diter les MÃ©ta-DonnÃ©es",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
