{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "type": "intro"
   },
   "source": [
    "Comme nous venons de le voir, Kedro est un outil puissant. Le premier pipeline que nous avons construit permettait d'encoder les donn√©es pour qu'elles soit utilis√©es par un mod√®le. Pour continuer √† construire le pipeline ML, nous allons d√©finir ici le **pipeline d'entra√Ænement de mod√®les**, qui pourra √™tre combin√© avec celui que nous avons d√©j√† fait sur l'encodage.\n",
    "\n",
    "<blockquote><p>üôã <b>Ce que nous allons faire</b></p>\n",
    "<ul>\n",
    "    <li>D√©velopper le pipeline d'entra√Ænement de mod√®les</li>\n",
    "    <li>Construire le pipeline ML de la collecte des donn√©es jusqu'√† la mod√©lisation</li>\n",
    "    <li>Appliquer les bonnes pratiques de d√©veloppement</li>\n",
    "</ul>\n",
    "</blockquote>\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l46CwEYnbFtFfjZNS/giphy.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entra√Ænement du mod√®le\n",
    "\n",
    "Nous avions appliqu√© de l'AutoML sur un LightGBM avec des Jupyter Notebooks. Notre objectif est maintenant de le faire directement dans le projet Kedro. Nous allons construire un pipeline nomm√© `training` qui est charg√© de de d√©terminer un mod√®le optimal.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Dans la construction, le pipeline <code>training</code> intervient apr√®s le pipeline <code>processing</code>, mais l'int√©r√™t de s√©parer le pipeline ML en deux est de pouvoir ex√©cuter un seul des deux pipelines au besoin.\n",
    "</div>\n",
    "\n",
    "Cr√©ons un dossier `training` dans `src/purchase_predict/pipelines`. Notre premier fichier `nodes.py` contient toutes les fonctions n√©cessaires pour entra√Æner le module. Pour am√©liorer la productivit√© et faciliter la maintenance du code, nous allons y cr√©er trois fonctions.\n",
    "\n",
    "- La premi√®re fonction `train_model` va entra√Æner une instance de mod√®le selon des hyper-param√®tres sp√©cifiques sur un ensemble d'entra√Ænement.\n",
    "- La seconde fonction `optimize_hyp` va d√©terminer les hyper-param√®tres qui maximisent le score d'un mod√®le √† partir d'une m√©trique sp√©cifique.\n",
    "- La derni√®re fonction `auto_ml` sera ensuite utilis√©e par le node Kedro pour entra√Æner un mod√®le optimis√©.\n",
    "\n",
    "Commen√ßons par importer les librairies n√©cessaires aux fonctions que nous d√©velopperons."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Callable, Tuple, Any, Dict\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "from hyperopt import hp, tpe, fmin\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cr√©ons une variable `MODELS` dans laquelle nous allons sp√©cifier une liste de mod√®les candidats. Pour commencer, nous n'utiliserons qu'un seul mod√®le LightGBM, mais nous pourrions √©galement calibrer d'autres mod√®les comme XGBoost, CatBoost, Random Forest. Nous d√©finissons un dictionnaire pour chaque mod√®le.\n",
    "\n",
    "- `name` est le nom du mod√®le.\n",
    "- `class` est l'objet Python permettant d'instancier le mod√®le.\n",
    "- `params` repr√©sente l'espace de recherche pour l'optimisation des hyper-param√®tres.\n",
    "- `override_schemas` est un champ qui contient les hyper-param√®tres dont le type doit √™tre modifi√© avant d'entra√Æner le mod√®le.\n",
    "\n",
    "Par exemple, l'hyper-param√®tre `max_depth` doit √™tre entier, mais un point de l'espace peut produire un flottant ($10.0$ au lieu de $10$). Dans ce cas, il faut forcer la conversion en entier pour √©viter une erreur g√©n√©r√©e par LightGBM."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "MODELS = [\n",
    "    {\n",
    "        \"name\": \"LightGBM\",\n",
    "        \"class\": LGBMClassifier,\n",
    "        \"params\": {\n",
    "            \"objective\": \"binary\",\n",
    "            \"verbose\": -1,\n",
    "            \"learning_rate\": hp.uniform(\"learning_rate\", 0.001, 1),\n",
    "            \"num_iterations\": hp.quniform(\"num_iterations\", 100, 1000, 20),\n",
    "            \"max_depth\": hp.quniform(\"max_depth\", 4, 12, 6),\n",
    "            \"num_leaves\": hp.quniform(\"num_leaves\", 8, 128, 10),\n",
    "            \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.3, 1),\n",
    "            \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "            \"min_child_samples\": hp.quniform(\"min_child_samples\", 1, 20, 10),\n",
    "            \"reg_alpha\": hp.choice(\"reg_alpha\", [0, 1e-1, 1, 2, 5, 10]),\n",
    "            \"reg_lambda\": hp.choice(\"reg_lambda\", [0, 1e-1, 1, 2, 5, 10]),\n",
    "        },\n",
    "        \"override_schemas\": {\n",
    "            \"num_leaves\": int,\n",
    "            \"min_child_samples\": int,\n",
    "            \"max_depth\": int,\n",
    "            \"num_iterations\": int,\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que le pipeline d'entra√Ænement du mod√®le ne poss√©dera qu'un node, nous pouvons tout √† fait cr√©er plusieurs fonctions dans le fichier `nodes.py` qui vont √™tre appel√©es par la fonction du node.\n",
    "\n",
    "Commen√ßons par la premi√®re fonction `train_model`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "def train_model(\n",
    "    instance: BaseEstimator,\n",
    "    training_set: Tuple[np.ndarray, np.ndarray],\n",
    "    params: Dict = {},\n",
    ") -> BaseEstimator:\n",
    "    \"\"\"\n",
    "    Trains a new instance of model with supplied training set and hyper-parameters.\n",
    "    \"\"\"\n",
    "    override_schemas = list(filter(lambda x: x[\"class\"] == instance, MODELS))[0][\n",
    "        \"override_schemas\"\n",
    "    ]\n",
    "    for p in params:\n",
    "        if p in override_schemas:\n",
    "            params[p] = override_schemas[p](params[p])\n",
    "    model = instance(**params)\n",
    "    model.fit(*training_set)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous l'avons √©voqu√© plus haut, nous for√ßons la conversion de certains hyper-param√®tres. On r√©cup√®re les noms des hyper-param√®tres dont le type est surcharg√©, puis ils sont convertis vers le type cible (ici, uniquement des entiers). Le reste de la fonction est explicitive, car il s'agit juste d'instancier le mod√®le et d'appeler la fonction `fit`.\n",
    "\n",
    "Voyons maintenant la deuxi√®me fonction `optimize_hyp`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "def optimize_hyp(\n",
    "    instance: BaseEstimator,\n",
    "    dataset: Tuple[np.ndarray, np.ndarray],\n",
    "    search_space: Dict,\n",
    "    metric: Callable[[Any, Any], float],\n",
    "    max_evals: int = 40,\n",
    ") -> BaseEstimator:\n",
    "    \"\"\"\n",
    "    Trains model's instances on hyper-parameters search space and returns most accurate\n",
    "    hyper-parameters based on eval set.\n",
    "    \"\"\"\n",
    "    X, y = dataset\n",
    "\n",
    "    def objective(params):\n",
    "        rep_kfold = RepeatedKFold(n_splits=4, n_repeats=1)\n",
    "        scores_test = []\n",
    "        for train_I, test_I in rep_kfold.split(X):\n",
    "            X_fold_train = X.iloc[train_I, :]\n",
    "            y_fold_train = y.iloc[train_I].values.flatten()\n",
    "            X_fold_test = X.iloc[test_I, :]\n",
    "            y_fold_test = y.iloc[test_I].values.flatten()\n",
    "            # On entra√Æne une instance du mod√®le avec les param√®tres params\n",
    "            model = train_model(\n",
    "                instance=instance,\n",
    "                training_set=(X_fold_train, y_fold_train),\n",
    "                params=params\n",
    "            )\n",
    "            # On calcule le score du mod√®le sur le test\n",
    "            scores_test.append(\n",
    "                metric(y_fold_test, model.predict(X_fold_test))\n",
    "            )\n",
    "\n",
    "        return np.mean(scores_test)\n",
    "\n",
    "    return fmin(fn=objective, space=search_space, algo=tpe.suggest, max_evals=max_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous r√©cup√©rons la base d'apprentissage $(X, y)$ √† partir de l'argument `dataset`. Nous d√©finissons ensuite la fonction `objective` dont on cherche une minimisation. √Ä chaque √©valuation, elle r√©alise un $k$-Fold sur le mod√®le candidat, puis stocke le score, calcul√©e √† partir de la m√©trique, dans la liste `scores_test`.\n",
    "\n",
    "Cette liste contient tous les scores sur les ensembles de test du $k$-Fold. Ici, nous aurons uniquement $4$ scores √† chaque ex√©cution de la fonction. Nous retournons donc le score moyen des mod√®les sur les ensembles de test.\n",
    "\n",
    "Pour terminer, nous retournons le r√©sultat de `fmin`, c'est-√†-dire le jeu d'hyper-param√®tres qui maximise la fonction `objective`.\n",
    "\n",
    "Enfin, la derni√®re fonction `auto_ml`, qui sera utilis√©e par le node Kedro."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "def auto_ml(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    max_evals: int = 40\n",
    ") -> BaseEstimator:\n",
    "    \"\"\"\n",
    "    Runs training of multiple model instances and select the most accurated based on objective function.\n",
    "    \"\"\"\n",
    "    X = pd.concat((X_train, X_test))\n",
    "    y = pd.concat((y_train, y_test))\n",
    "\n",
    "    opt_models = []\n",
    "    for model_specs in MODELS:\n",
    "        # Finding best hyper-parameters with bayesian optimization\n",
    "        optimum_params = optimize_hyp(\n",
    "            model_specs[\"class\"],\n",
    "            dataset=(X, y),\n",
    "            search_space=model_specs[\"params\"],\n",
    "            metric=lambda x, y: -f1_score(x, y),\n",
    "            max_evals=max_evals\n",
    "        )\n",
    "        print(\"done\")\n",
    "        # Training the supposed best model with found hyper-parameters\n",
    "        model = train_model(\n",
    "            model_specs[\"class\"],\n",
    "            training_set=(X_train, y_train),\n",
    "            params=optimum_params,\n",
    "        )\n",
    "        opt_models.append(\n",
    "            {\n",
    "                \"model\": model,\n",
    "                \"name\": model_specs[\"name\"],\n",
    "                \"params\": optimum_params,\n",
    "                \"score\": f1_score(y_test, model.predict(X_test)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # In case we have multiple models\n",
    "    best_model = max(opt_models, key=lambda x: x[\"score\"])\n",
    "    return dict(model=best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit simplement d'une ex√©uction s√©quenc√©e en plusieurs √©tapes. Pour chaque mod√®le candidat que l'on souhaite optimiser, il y a trois √©tapes.\n",
    "\n",
    "- On d√©termine les hyper-param√®tres optimaux avec `optimize_hyp` sur la base d'apprentissage $(X, y)$.\n",
    "- Une fois les hyper-param√®tres optimaux trouv√©s, une instance du mod√®le est calibr√© avec ces derniers sur le sous-√©chantillon d'entra√Ænement.\n",
    "- Le score du mod√®le est calcul√© sur le sous-√©chantillon de test et stock√© dans une liste.\n",
    "\n",
    "√Ä la fin, la liste `opt_models` contiendra tous les mod√®les optimis√© de chaque mod√®le candidat. Si l'on souhaite tester un LightGBM, un XGBoost et un CatBoost, alors `opt_models` contiendra trois √©l√©ments, chacun √©tant le mod√®le optimis√© de la classe d'algorithme.\n",
    "\n",
    "Pour terminer, on tourne le meilleur mod√®le parmi tous les mod√®les optimis√©s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cr√©ation du pipeline\n",
    "\n",
    "Derni√®re √©tape : construire le pipeline dans le fichier `pipeline.py`. Avant toute chose, nous allons d√©finir le mod√®le (optimis√©) dans le Data Catalog afin de l'enregistrer une fois entra√Æn√©. On utilise le type `pickle.PickleDataSet` pour enregistrer un mod√®le au format binaire sur le disque."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "yaml"
   },
   "source": [
    "model:\n",
    "  type: pickle.PickleDataSet\n",
    "  filepath: data/06_models/model.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De m√™me, nous allons d√©finir un param√®re Kedro pour le nombre d'it√©rations lors de la phase d'AutoML."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "yaml"
   },
   "source": [
    "test_ratio: 0.3\n",
    "automl_max_evals: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le pipeline, il n'y a qu'un seul node √† d√©finir."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from kedro.pipeline import Pipeline, node\n",
    "\n",
    "from .nodes import auto_ml\n",
    "\n",
    "def create_pipeline(**kwargs):\n",
    "    return Pipeline(\n",
    "        [\n",
    "            node(\n",
    "                auto_ml,\n",
    "                [\n",
    "                    \"X_train\",\n",
    "                    \"y_train\",\n",
    "                    \"X_test\",\n",
    "                    \"y_test\",\n",
    "                    \"params:automl_max_evals\"\n",
    "                ],\n",
    "                dict(model=\"model\"),\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On installera les packages n√©cessaires avec `pip install lightgbm hyperopt` en v√©rifiant que l'on soit bien dans l'environnement virtuel.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    Pour mettre √† jour la visualisation des pipelines, il faut relancer le processus <code>kedro viz</code> dans un terminal (on le stoppera au pr√©alable avec <code>Ctrl+C</code>).\n",
    "</div>\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/kedro3.png\" />\n",
    "\n",
    "Comme nous l'avions fait pour le pipeline `processing`, nous devons d√©finir le pipeline dans le fichier `hooks.py`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from purchase_predict.pipelines.training import pipeline as training_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apr√®s l'importation du pipeline, mettons √† jour le corps de la fonction `register_pipelines`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "def register_pipelines(self) -> Dict[str, Pipeline]:\n",
    "    \"\"\"Register the project's pipeline.\n",
    "\n",
    "    Returns:\n",
    "        A mapping from a pipeline name to a ``Pipeline`` object.\n",
    "\n",
    "    \"\"\"\n",
    "    p_processing = processing_pipeline.create_pipeline()\n",
    "    p_training = training_pipeline.create_pipeline()\n",
    "\n",
    "    return {\n",
    "        \"processing\": p_processing,\n",
    "        \"training\": p_training\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'√† ex√©cuter le pipeline. Nous allons au total entra√Æner 40 mod√®les puisque nous avons un $4$-Fold et 10 it√©rations : le temps d'ex√©cution sera d'environ 3 √† 5 minutes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "kedro run --pipeline training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "console"
   },
   "source": [
    "2021-01-06 09:26:25,788 - hyperopt.tpe - INFO - build_posterior_wrapper took 0.002988 seconds\n",
    "2021-01-06 09:26:25,788 - hyperopt.tpe - INFO - TPE using 4/4 trials with best loss -0.867137\n",
    " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 5/10 [03:31<03:51, 46.34s/trial, best loss: -0.867873849887163]2021-01-06 09:27:04,188 - hyperopt.tpe - INFO - build_posterior_wrapper took 0.002868 seconds\n",
    "2021-01-06 09:27:04,189 - hyperopt.tpe - INFO - TPE using 5/5 trials with best loss -0.867874\n",
    " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 6/10 [03:53<02:32, 38.03s/trial, best loss: -0.8825673114362647]2021-01-06 09:27:26,089 - hyperopt.tpe - INFO - build_posterior_wrapper took 0.002958 seconds\n",
    "2021-01-06 09:27:26,089 - hyperopt.tpe - INFO - TPE using 6/6 trials with best loss -0.882567\n",
    " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 7/10 [03:57<01:21, 27.01s/trial, best loss: -0.8825673114362647]2021-01-06 09:27:30,472 - hyperopt.tpe - INFO - build_posterior_wrapper took 0.002919 seconds\n",
    "2021-01-06 09:27:30,472 - hyperopt.tpe - INFO - TPE using 7/7 trials with best loss -0.882567\n",
    " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 8/10 [04:07<00:42, 21.46s/trial, best loss: -0.8825673114362647]2021-01-06 09:27:39,979 - hyperopt.tpe - INFO - build_posterior_wrapper took 0.003218 seconds\n",
    "2021-01-06 09:27:39,979 - hyperopt.tpe - INFO - TPE using 8/8 trials with best loss -0.882567\n",
    " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 9/10 [04:22<00:19, 19.57s/trial, best loss: -0.8825673114362647]2021-01-06 09:27:55,385 - hyperopt.tpe - INFO - build_posterior_wrapper took 0.002981 seconds\n",
    "2021-01-06 09:27:55,386 - hyperopt.tpe - INFO - TPE using 9/9 trials with best loss -0.882567\n",
    "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [04:39<00:00, 27.93s/trial, best loss: -0.8825673114362647]\n",
    "done\n",
    "2021-01-06 09:28:17,086 - kedro.io.data_catalog - INFO - Saving data to `model` (PickleDataSet)...\n",
    "2021-01-06 09:28:17,640 - kedro.runner.sequential_runner - INFO - Completed 1 out of 1 tasks\n",
    "2021-01-06 09:28:17,640 - kedro.runner.sequential_runner - INFO - Pipeline execution completed successfully.\n",
    "2021-01-06 09:28:17,640 - kedro.framework.session.store - INFO - `save()` not implemented for `BaseSessionStore`. Skipping the step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "break": "new"
   },
   "source": [
    "## Pipeline de chargement\n",
    "\n",
    "Dans le pipeline `processing`, on suppose que le jeu de donn√©es `primary.csv` existe d√©j√†. Or, il s'agit pour l'instant du jeu de donn√©es **de l'√©chantillon**, et non de celui calcul√© sur un historique de 7 jours. Nous devons au pr√©alable cr√©er un troisi√®me pipeline qui interviendra en tout premier.\n",
    "\n",
    "Cr√©ons un dossier `loading` avec les fichiers `nodes.py` et `pipeline.py` pour le pipeline de m√™me nom."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def load_csv_from_bucket(project: str, bucket_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads multiple CSV files from bucket (as generated from Spark).\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name = bucket_path.split(\"/\")[0]\n",
    "    folder = \"/\".join(bucket_path.split(\"/\")[1:]) + \"/part-\"\n",
    "\n",
    "    for blob in storage_client.list_blobs(bucket_name, prefix=folder):\n",
    "        filename = blob.name.split(\"/\")[-1]\n",
    "        if filename[-3:] == \"csv\":\n",
    "            blob.download_to_filename(\"/tmp/\" + filename)\n",
    "\n",
    "    all_files = glob.glob(\"/tmp/*.csv\")\n",
    "    li = []\n",
    "\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, sep=\",\")\n",
    "        li.append(df)\n",
    "\n",
    "    df = pd.concat(li, axis=0, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L√†-aussi, il nous faut installer `pip install google-cloud-storage` dans l'environnement virtuel.\n",
    "\n",
    "Avec `storage_client`, nous t√©l√©chargeons en local dans le dossier `/tmp` (dossier de fichiers temporaires) tous les fichiers CSV dans le bucket sp√©cifi√©. √Ä l'aide de `glob`, nous parcourons tous ces fichiers t√©l√©charg√©s et les concat√©nons dans un seul et unique DataFrame `df`.\n",
    "\n",
    "Puisqu'il n'y a qu'un seul noeud, le pipeline est rapide √† d√©finir."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from kedro.pipeline import Pipeline, node\n",
    "\n",
    "from .nodes import load_csv_from_bucket\n",
    "\n",
    "def create_pipeline(**kwargs):\n",
    "    return Pipeline(\n",
    "        [\n",
    "            node(\n",
    "                load_csv_from_bucket,\n",
    "                [\"params:gcp_project_id\", \"params:gcs_primary_folder\"],\n",
    "                \"primary\",\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons rajouter deux param√®tres dans le fichier `parameters.yml`.\n",
    "\n",
    "- Le param√®tre `gcp_project_id` contenant le nom du projet Google Cloud.\n",
    "- Le param√®tre `gcs_primary_folder` qui donne **le dossier** contenant les fichiers CSV transform√©s, pr√™ts √† √™tre encod√©s."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "yaml"
   },
   "source": [
    "gcp_project_id: \"<PROJET_GCP>\"\n",
    "gcs_primary_folder: \"<NOM_DU_BUCKET>/primary/data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme toujours, pour rajouter le pipeline, il faut configurer le fichier `hooks.py`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from purchase_predict.pipelines.loading import pipeline as loading_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons donc un total de trois pipelines."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "def register_pipelines(self) -> Dict[str, Pipeline]:\n",
    "    \"\"\"Register the project's pipeline.\n",
    "\n",
    "    Returns:\n",
    "        A mapping from a pipeline name to a ``Pipeline`` object.\n",
    "\n",
    "    \"\"\"\n",
    "    p_processing = processing_pipeline.create_pipeline()\n",
    "    p_training = training_pipeline.create_pipeline()\n",
    "    p_loading = loading_pipeline.create_pipeline()\n",
    "\n",
    "    return {\n",
    "        \"loading\": p_loading,\n",
    "        \"processing\": p_processing,\n",
    "        \"training\": p_training\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/pipeline_ml1.png\" />\n",
    "\n",
    "Essayons d'ex√©cuter le pipeline `loading`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "kedro run --pipeline loading"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "console"
   },
   "source": [
    "  File \"/home/jovyan/purchase_predict/venv/lib/python3.8/site-packages/fsspec/asyn.py\", line 121, in wrapper\n",
    "    return maybe_sync(func, self, *args, **kwargs)\n",
    "  File \"/home/jovyan/purchase_predict/venv/lib/python3.8/site-packages/fsspec/asyn.py\", line 100, in maybe_sync\n",
    "    return sync(loop, func, *args, **kwargs)\n",
    "  File \"/home/jovyan/purchase_predict/venv/lib/python3.8/site-packages/fsspec/asyn.py\", line 71, in sync\n",
    "    raise exc.with_traceback(tb)\n",
    "  File \"/home/jovyan/purchase_predict/venv/lib/python3.8/site-packages/fsspec/asyn.py\", line 55, in f\n",
    "    result[0] = await future\n",
    "  File \"/home/jovyan/purchase_predict/venv/lib/python3.8/site-packages/gcsfs/core.py\", line 808, in _ls\n",
    "    out = await self._list_objects(path)\n",
    "  File \"/home/jovyan/purchase_predict/venv/lib/python3.8/site-packages/gcsfs/core.py\", line 598, in _list_objects\n",
    "    items, prefixes = await self._do_list_objects(path)\n",
    "  File \"/home/jovyan/purchase_predict/venv/lib/python3.8/site-packages/gcsfs/core.py\", line 626, in _do_list_objects\n",
    "    page = await self._call(\n",
    "  File \"/home/jovyan/purchase_predict/venv/lib/python3.8/site-packages/gcsfs/core.py\", line 525, in _call\n",
    "    raise e\n",
    "  File \"/home/jovyan/purchase_predict/venv/lib/python3.8/site-packages/gcsfs/core.py\", line 507, in _call\n",
    "    self.validate_response(status, contents, json, path, headers)\n",
    "  File \"/home/jovyan/purchase_predict/venv/lib/python3.8/site-packages/gcsfs/core.py\", line 1228, in validate_response\n",
    "    raise HttpError(error)\n",
    "gcsfs.utils.HttpError: Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La description de l'erreur est explicite : `Anonymous caller does not have storage.objects.list access` car nous **ne sommes pas authentifi√©**. Afin d'authentifier une application pour acc√©der √† une ou plusieurs ressources Google Cloud, nous devons cr√©er un <a href=\"https://console.cloud.google.com/identity/serviceaccounts?project=training-ml-engineer\" target=\"_blank\">compte de service</a>.\n",
    "\n",
    "### Cr√©ation d'un compte de service\n",
    "\n",
    "Une r√®gle de s√©curit√© en Cloud Computing consiste √† cr√©er des **r√¥les** en fonction des services et des cas d'utilisation. Sous GCP, les **comptes de services** permettent de d√©finir des r√¥les aux applications qui vont faire appel √† certains services, avec un certain niveau d'habilitation tout en garantissant une s√©curit√© √©lev√©e.\n",
    "\n",
    "Nous allons nommer ce compte de service `purchase-predict` avec le r√¥le *Lecteur des objets de l'espace de stockage*, ce qui autorisera la lecture des donn√©es depuis l'ext√©rieur.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/pipeline_ml2.png\" />\n",
    "\n",
    "La description du compte de service n'est pas obligatoire, mais elle permet de pr√©senter rapidement quelles applications vont utiliser ce compte de service car cela n'est pas toujours clair lorsqu'il y en a beaucoup.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/pipeline_ml3.png\" />\n",
    "\n",
    "En s√©curit√© du Cloud, il faut adopter le **principe du moindre privil√®ge** : il faut donner uniquement les acc√®s n√©cessaires, c'est-√†-dire juste ce qu'il faut pour que l'application fonctionne, jamais plus.\n",
    "\n",
    "Apr√®s avoir cr√©e le compte de service, nous pouvons cr√©er une cl√© au format JSON et la t√©l√©charger.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    La cl√© du compte de service est √©quivalent √† un mot de passe. Il ne faut jamais la divulguer ou la laisser dans un projet o√π Git ajouterai ce fichier √† un d√©p√¥t distant.\n",
    "</div>\n",
    "\n",
    "Heureusement pour nous, Kedro a pr√©vu de cas de figure. Tous les mots de passes et cl√©s de services, lorsqu'ils sont utilis√© sur des environnement de d√©veloppement, doivent √™tre plac√©s dans le dossier `conf/local`, qui est ignor√© par Git. Ajoutons le fichier `service-account.json` dans ce dossier en ins√©rant le contenu de la cl√© JSON t√©l√©charg√©e.\n",
    "\n",
    "Il reste maintenant √† sp√©cifier le chemin d'acc√®s √† la cl√© du compte de service dans la variable d'environnement `GOOGLE_APPLICATION_CREDENTIALS`. √Ä noter que cette manipulation sera n√©cessaire **lorsque l'on d√©marre un nouveau terminal**."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "export GOOGLE_APPLICATION_CREDENTIALS=\"conf/local/service-account.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons d'ex√©cuter √† nouveau le pipeline."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "kedro run --pipeline loading"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "console"
   },
   "source": [
    "2021-01-06 13:12:49,072 - root - INFO - ** Kedro project purchase_predict\n",
    "2021-01-06 13:12:49,090 - kedro.io.data_catalog - INFO - Loading data from `params:gcp_project_id` (MemoryDataSet)...\n",
    "2021-01-06 13:12:49,091 - kedro.io.data_catalog - INFO - Loading data from `params:gcs_primary_folder` (MemoryDataSet)...\n",
    "2021-01-06 13:12:49,091 - kedro.pipeline.node - INFO - Running node: load_csv_from_bucket([params:gcp_project_id,params:gcs_primary_folder]) -> [primary]\n",
    "2021-01-06 13:12:51,814 - kedro.io.data_catalog - INFO - Saving data to `primary` (CSVDataSet)...\n",
    "2021-01-06 13:12:53,173 - kedro.runner.sequential_runner - INFO - Completed 1 out of 1 tasks\n",
    "2021-01-06 13:12:53,173 - kedro.runner.sequential_runner - INFO - Pipeline execution completed successfully.\n",
    "2021-01-06 13:12:53,174 - kedro.framework.session.store - INFO - `save()` not implemented for `BaseSessionStore`. Skipping the step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout a bien fonctionn√© : le fichier `primary.csv` correspond bien au jeu de donn√©es construit avec PySpark.\n",
    "\n",
    "Au total, nous comptabilisons trois pipelines.\n",
    "\n",
    "- Le pipeline `loading` qui charger les fichiers CSV depuis le bucket Cloud Storage pour cr√©er le fichier `primary.csv`.\n",
    "- Le pipeline `processing` qui va encoder le jeu de donn√©es `primary.csv`.\n",
    "- Le pipeline `training` qui va entra√Æner un mod√®le.\n",
    "\n",
    "L'avantage de la repr√©sentation de ces trois pipelines est **la flexibilit√©**. Si je souhaite ajouter un XGBoost par exemple, je n'aurai besoin que de relance le pipeline `training`. Si je change de m√©thodes d'encodages, je n'aurai pas besoin de relancer le pipeline `loading`. L'autre aspect important est **l'homog√©n√©it√© des traitements** : en regroupant toutes les op√©rations dans trois pipelines, nous nous assurons que les donn√©es au tout d√©but de la cha√Æne vont subir exactement le m√™me traitement, quelle que soit la date d'ex√©cution ou les donn√©es en entr√©e.\n",
    "\n",
    "Nous pouvons alors cr√©er un pipeline `global`, qui est la fusion des trois pipelines s√©quenc√©s.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/pipeline_ml4.png\" />"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "def register_pipelines(self) -> Dict[str, Pipeline]:\n",
    "    \"\"\"Register the project's pipeline.\n",
    "\n",
    "    Returns:\n",
    "        A mapping from a pipeline name to a ``Pipeline`` object.\n",
    "\n",
    "    \"\"\"\n",
    "    p_processing = processing_pipeline.create_pipeline()\n",
    "    p_training = training_pipeline.create_pipeline()\n",
    "    p_loading = loading_pipeline.create_pipeline()\n",
    "\n",
    "    return {\n",
    "        \"global\": Pipeline([p_loading, p_processing, p_training]),\n",
    "        \"loading\": p_loading,\n",
    "        \"processing\": p_processing,\n",
    "        \"training\": p_training\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons ensuite lancer le pipeline `global`. √Ä noter que le pipeline `training` prendra plus de temps puisque le jeu de donn√©es est plus cons√©quent."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "kedro run --pipeline global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "break": "new"
   },
   "source": [
    "## Linting et Refactoring\n",
    "\n",
    "√Ä partir de maintenant, les codes qui vont √™tre produits seront utilis√©s dans des environnements de production. Il est **n√©cessaire** que le code respecte les normes de PEP 8, telles que nous les avions vues avec `flake8` et `black`. Installons ces deux packages."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "pip install flake8 black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ensuite cr√©er deux fichiers.\n",
    "\n",
    "- Le fichier `.flake8` √† la racine du projet configure les param√®tres propres √† `flake8`. En particulier, nous pouvons par exemple ignorer certaines erreurs de PEP 8 ou, √† l'inverse, ajouter ses propres sp√©cifications.\n",
    "- Le fichier `.toml`, lui aussi √† la racine du projet, configure les param√®tres pour `black`. √Ä noter que dans les deux fichiers, il est important de pr√©ciser la m√™me taille pour les lignes. L√† o√π `flake8` g√©n√©rera une erreur, `black` d√©coupera la ligne actuelle en plusieurs.\n",
    "\n",
    "Commen√ßons par le fichier `.flake8`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "plaintext"
   },
   "source": [
    "[flake8]\n",
    "max-line-length = 120\n",
    "max-complexity = 16\n",
    "exclude = .git,.ipython,__pycache__,venv,build,dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous choisissons volontairement un nombre de caract√®res maximal √† 120. La complexit√©, qui est un calcul r√©alis√© en fonction du nombre d'identations maximales et de variables utilis√©es, est fix√©e √† 16. Enfin, nous d√©finissons les dossiers et fichiers √† exclure de l'analyse.\n",
    "\n",
    "Le contenu du fichier `.toml` est tr√®s similaire."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "plaintext"
   },
   "source": [
    "[tool.black]\n",
    "line-length = 120\n",
    "include = '\\.pyi?$'\n",
    "exclude = '''\n",
    "/(\n",
    "    \\.git\n",
    "  | venv\n",
    "  | build\n",
    "  | dist\n",
    "  | conf\n",
    ")/\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seule diff√©rence est que l'on choisit √©galement d'y inclure certains fichiers dont l'extension est `.pyi`. Ex√©cutons tout d'abord `flake8`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "console"
   },
   "source": [
    "./src/purchase_predict/hooks.py:42:1: E302 expected 2 blank lines, found 1\n",
    "./src/purchase_predict/pipelines/training/pipeline.py:5:1: E302 expected 2 blank lines, found 1\n",
    "./src/purchase_predict/pipelines/training/pipeline.py:20:6: W292 no newline at end of file\n",
    "./src/purchase_predict/pipelines/training/nodes.py:43:1: E302 expected 2 blank lines, found 1\n",
    "./src/purchase_predict/pipelines/training/nodes.py:61:1: E302 expected 2 blank lines, found 1\n",
    "./src/purchase_predict/pipelines/training/nodes.py:96:1: E302 expected 2 blank lines, found 1\n",
    "./src/purchase_predict/pipelines/training/nodes.py:137:34: W292 no newline at end of file\n",
    "./src/purchase_predict/pipelines/loading/pipeline.py:8:1: E302 expected 2 blank lines, found 1\n",
    "./src/purchase_predict/pipelines/loading/pipeline.py:17:6: W292 no newline at end of file\n",
    "./src/purchase_predict/pipelines/loading/nodes.py:9:1: E302 expected 2 blank lines, found 1\n",
    "./src/purchase_predict/pipelines/loading/nodes.py:27:14: W292 no newline at end of file\n",
    "./src/purchase_predict/pipelines/processing/pipeline.py:8:1: E302 expected 2 blank lines, found 1\n",
    "./src/purchase_predict/pipelines/processing/pipeline.py:27:6: W292 no newline at end of file\n",
    "./src/purchase_predict/pipelines/processing/nodes.py:10:1: E302 expected 2 blank lines, found 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons que la majorit√© des erreurs et avertissements peuvent √™tre corrig√©s par un formatage de code avec `black`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "black ."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "console"
   },
   "source": [
    "reformatted /home/jovyan/purchase_predict/src/purchase_predict/pipelines/loading/pipeline.py\n",
    "reformatted /home/jovyan/purchase_predict/src/purchase_predict/hooks.py\n",
    "reformatted /home/jovyan/purchase_predict/src/purchase_predict/pipelines/loading/nodes.py\n",
    "reformatted /home/jovyan/purchase_predict/src/purchase_predict/pipelines/processing/pipeline.py\n",
    "reformatted /home/jovyan/purchase_predict/src/purchase_predict/pipelines/processing/nodes.py\n",
    "reformatted /home/jovyan/purchase_predict/src/purchase_predict/pipelines/training/pipeline.py\n",
    "reformatted /home/jovyan/purchase_predict/src/setup.py\n",
    "reformatted /home/jovyan/purchase_predict/src/purchase_predict/pipelines/training/nodes.py\n",
    "All done! ‚ú® üç∞ ‚ú®\n",
    "8 files reformatted, 10 files left unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ex√©cutant √† nouveau `flake8`, il ne devrait y avoir aucune sortie console, signifiant que tous les codes sources respectent PEP 8.\n",
    "\n",
    "> ‚ùì Est-ce que l'on doit tout le temps ex√©cuter ces deux commandes ?\n",
    "\n",
    "Les bonnes pratiques en d√©veloppement logiciel, c'est de toujours publier un code qui respecte au maximum les normes, notamment PEP 8 dans le cas de Python. Le plus ad√©quat ici serait d'ex√©cuter automatiquement `black` puis `flake8` √† chaque commit de Git. Il y aurait alors deux possibilit√©s.\n",
    "\n",
    "- Le code ne respecte pas la norme PEP 8, et le commit n'est pas accept√©.\n",
    "- Le code respecte la norme PEP 8, et le commit est accept√© **en local**.\n",
    "\n",
    "Il faudra ensuite pousser le code local vers le d√©p√¥t distant. Et tout ceci peut √™tre r√©alis√© avec `pre-commit`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "pip install pre-commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit d'un utilitaire qui permet d'ex√©cuter des traitements avant, pendant et apr√®s des actions Git (lors d'un commit, d'un push, etc). Pour cela, nous configurons les √©v√©nements √† surveiller en sp√©cifiant, dans le fichier `.pre-commit-config.yaml`, toujours √† la racine du projet."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "plaintext"
   },
   "source": [
    "repos:\n",
    "-   repo: https://github.com/ambv/black\n",
    "    rev: stable\n",
    "    hooks:\n",
    "    - id: black\n",
    "      language_version: python3.8\n",
    "-   repo: https://gitlab.com/pycqa/flake8\n",
    "    rev: 3.8.4\n",
    "    hooks:\n",
    "    - id: flake8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Pour initialiser l'environnement et installer les d√©clencheurs, il suffit d'ex√©cuter `pre-commit install` dans la console. Cette op√©ration n√©cessite quelques dizaines de secondes. Dor√©navant, √† chaque commit, `black` puis `flake8` sont ex√©cut√©s et cela garantit que le code qui sera par la suite pouss√© vers le d√©p√¥t distant respecte la norme PEP 8."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "git add .\n",
    "git commit -am \"Added linting and refactoring\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "console"
   },
   "source": [
    "black................................................(no files to check)Skipped\n",
    "flake8...............................................(no files to check)Skipped\n",
    "On branch master\n",
    "nothing to commit, working tree clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dor√©navant, √† la moindre modification de fichier, lors d'un commit, tous les codes Python seront inspect√©s."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "console"
   },
   "source": [
    "black....................................................................Passed\n",
    "flake8...................................................................Passed\n",
    "[master c6ff4a6] Added linting and refactoring\n",
    " 1 file changed, 1 deletion(-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour terminer, nous pouvons pousser le code vers le d√©p√¥t distant."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "git push google master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√Ä noter qu'√† chaque nouveau terminal, il faut relancer l'agent SSH et lui sp√©cifier la cl√©e priv√©e pour pouvoir avoir les droits sur le d√©p√¥t distant."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "eval \"$(ssh-agent -s)\"\n",
    "chmod 600 ~/ssh/git_key\n",
    "ssh-add ~/ssh/git_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "type": "ending"
   },
   "source": [
    "## ‚úîÔ∏è Conclusion\n",
    "\n",
    "Notre pipeline ML est enfin r√©alis√© avec Kedro.\n",
    "\n",
    "- Tout d'abord, nous avons construit le pipeline qui entra√Æne des mod√®les.\n",
    "- Ensuite, nous avons combin√© tous les pipelines (collecte, encodage et entra√Ænement) en un seul.\n",
    "- Pour finir, nous avons appliqu√© les bonnes pratiques de d√©veloppement en v√©rifiant les codes Python avec la norme PEP 8.\n",
    "\n",
    "> ‚û°Ô∏è Au programme des prochaines activit√©s : les **d√©p√¥ts de mod√®les** ! Mais avant, il est n√©cessaire d'aborder un point pas toujours plaisant, mais pour autant tr√®s important : les **tests logiciels**."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "√âditer les M√©ta-Donn√©es",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
