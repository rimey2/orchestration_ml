{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "type": "intro"
   },
   "source": [
    "Jusqu'√† maintenant, nous avons vu avec MLflow la puissance du composant Tracking, qui permettait de fournir un historique des exp√©rimentations r√©alis√©es en gardant une tracabilit√© des hyper-param√®tres utilis√©s, des m√©triques obtenus mais √©galement des artifacts g√©n√©r√©s (mod√®le, graphique).\n",
    "\n",
    "Il y a √©galement un autre composant particuli√®rement utile en tant que ML Engineer : c'est le **registre de mod√®le**. √Ä l'instar d'un d√©p√¥t Git, le registre de mod√®le MLflow permettra de g√©rer plusieurs versions de mod√®les en leur attribuant √©galement des tags, qui correspondent √† des environnement de pr√©-production (*staging*) et de production.\n",
    "\n",
    "<blockquote><p>üôã <b>Ce que nous allons faire</b></p>\n",
    "<ul>\n",
    "    <li>Int√©grer le tracking MLflow dans Kedro</li>\n",
    "    <li>Cr√©er un mod√®le packag√© sous MLflow</li>\n",
    "    <li>Versioner les mod√®les dans Kedro</li>\n",
    "</ul>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking MLflow sous Kedro\n",
    "\n",
    "Dans le Notebook MLflow, nous avions entra√Æn√© un LightGBM manuellement et l'avons envoy√© directement sur MLflow et Cloud Storage. L'objectif ici est de modifier le pipeline `training` sous Kedro pour y ajouter l'int√©gration avec MLflow.\n",
    "\n",
    "Commen√ßons par ajouter une variable d'environnement. Sous Python, le package `python-dotenv` est utile pour r√©cup√©rer les variables d'environnement ou pour en configurer automatiquement lors du d√©veloppement du projet."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "pip install python-dotenv mlflow google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√Ä la racine du projet, cr√©ons le fichier `.env`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "plaintext"
   },
   "source": [
    "# Remplacer par l'adresse IP de la VM contenant MLflow\n",
    "MLFLOW_SERVER=http://XX.XX.XX.XX/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le r√¥le de ce fichier est identique √† un `export` qui serait r√©alis√© sur chaque variable. Le principal int√©r√™t, en plus de pouvoir centraliser toutes les variables d'environnement, est que `python-dotenv` supporte aussi les variables d'environnement : il peut donc √™tre utilis√© √† la fois lors du d√©veloppement ou une fois le code en production.\n",
    "\n",
    "√Ä noter que nous aurions pu utiliser le fichier `parameters.yml` de Kedro. Dans la pratique, ce fichier est plut√¥t r√©serv√© aux param√®tres des pipelines (ratio pour l'ensemble de test, hyper-param√®tres par d√©faut, etc), et il est pr√©f√©rable, comme c'est le cas en d√©veloppement logiciel, d'inscrire des param√®tres plus g√©n√©raux (comme les r√©f√©rences aux servers, l'environnement de pr√©-production ou de production) dans des fichiers de configuration ou des variables d'environnement.\n",
    "\n",
    "Cr√©ons le fichier `__init__.py` dans le dossier `training` des pipelines."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour rappel, ce fichier sera ex√©cut√© initialement par l'interpr√©teur Python, permettant ainsi de d√©finir des configurations qui seront partag√©es par tous les fichiers du module/dossier. Par la suite, nous allons pouvoir appeler `os.getenv(\"MLFLOW_SERVER\")` pour r√©cup√©rer cette variable d'environnement dans n'importe quel fichier du dossier contenant ce `__init__.py`.\n",
    "\n",
    "Nous allons ajouter deux param√®tres dans `parameters.yml`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "yaml"
   },
   "source": [
    "mlflow_enabled: True # Do we log metrics and artifacts to MLflow ?\n",
    "mlflow_experiment_id: 1 # Experimented ID associated to this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela permet de d√©cider √† tout moment si l'on souhaite envoyer les logs et artifacts vers MLflow, ainsi que l'identifiant de l'exp√©rience MLflow.\n",
    "\n",
    "Dans `nodes.py` de `training`, modifiant la fonction `auto_ml` pour accepter ces deux nouveaux param√®tres."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "def auto_ml(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    max_evals: int = 40,\n",
    "    log_to_mlflow: bool = False,\n",
    "    experiment_id: int = -1,\n",
    ") -> BaseEstimator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par d√©faut, si aucune information n'est sp√©cifi√©e concernant MLflow, on pr√©f√®re ne pas envoyer de logs ou d'artifacts. Importons `os` ainsi que `mlflow`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au tout d√©but de la fonction, apr√®s avoir r√©cup√©r√© la base d'apprentissage $(X, y)$, nous d√©marrons un run."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "X = pd.concat((X_train, X_test))\n",
    "y = pd.concat((y_train, y_test))\n",
    "\n",
    "run_id = \"\"\n",
    "if log_to_mlflow:\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_SERVER\"))\n",
    "    run = mlflow.start_run(experiment_id=experiment_id)\n",
    "    run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si l'on souhaite tracker avec MLflow (`log_to_mlflow` √† `True`), alors on sp√©cifie l'URL de tracking, puis on lance un run avec `start_run` en indiquant l'identifiant de l'exp√©rience associ√© au run.\n",
    "\n",
    "√Ä la fin de la fonction, nous envoyons les logs et artifacts vers MLflow. Elle retournera √©galement le champ `mlflow_run_id`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "# In case we have multiple models\n",
    "best_model = max(opt_models, key=lambda x: x[\"score\"])\n",
    "\n",
    "if log_to_mlflow:\n",
    "    model_metrics = {\n",
    "        \"f1\": best_model[\"score\"]\n",
    "    }\n",
    "\n",
    "    mlflow.log_metrics(model_metrics)\n",
    "    mlflow.log_params(optimum_params)\n",
    "    # Only use if validation curves are produced\n",
    "    mlflow.log_artifacts(\"data/08_reporting\", artifact_path=\"plots\")\n",
    "    mlflow.sklearn.log_model(best_model[\"model\"], \"model\")\n",
    "    mlflow.end_run()\n",
    "    \n",
    "return dict(model=best_model, mlflow_run_id=run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derni√®re modification √† apporter est de mettre √† jour la d√©finition du pipeline."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "def create_pipeline(**kwargs):\n",
    "    return Pipeline(\n",
    "        [\n",
    "            node(\n",
    "                auto_ml,\n",
    "                [\n",
    "                    \"X_train\", \"y_train\", \"X_test\", \"y_test\",\n",
    "                    \"params:automl_max_evals\", \"params:mlflow_enabled\",\n",
    "                    \"params:mlflow_experiment_id\"],\n",
    "                dict(model=\"model\", mlflow_run_id=\"mlflow_run_id\"),\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/model_registry3.png\" />\n",
    "\n",
    "Avant d'ex√©cuter le pipeline, il faut penser √† donner les droits d'√©criture sur Cloud Storage au compte de service de Kedro. Rappelons-nous, nous avions cr√©e un compte de service pour pouvoir r√©cup√©rer les fichiers CSV sur le bucket. Pas besoin d'en cr√©er un nouveau : nous pouvons utiliser celui existant en lui rajoutant de nouvelles autorisations.\n",
    "\n",
    "Dirigeons-nous dans <a href=\"https://console.cloud.google.com/iam-admin/iam\" target=\"_blank\">IAM</a> et √©ditons le membre `purchase-predict@...`.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/model_registry1.png\" />\n",
    "\n",
    "Ajoutons le r√¥le Cr√©ateur des objets de l'espace de stockage. Apr√®s modification des r√¥les, il faut ret√©l√©charger une cl√© pour <a href=\"https://console.cloud.google.com/iam-admin/serviceaccounts\" target=\"_blank\">le compte de service</a>. Une fois le contenu de la cl√© modifi√© dans `conf/local/service-account.json`, nous devons mettre √† jour la variable d'environnement `GOOGLE_APPLICATION_CREDENTIALS`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "export GOOGLE_APPLICATION_CREDENTIALS=\"conf/local/service-account.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√Ä noter que pour aller plus vite, nous pouvons d√©finir cette variable dans le fichier `.env`.\n",
    "\n",
    "Temporairement, pour v√©rifier que notre code fonctionne, nous allons modifier le param√®tre `automl_max_evals` √† $1$. Dans le m√™me temps, mettons en place le pipeline `global` qui va ex√©cuter de mani√®re s√©quentielle les trois pipelines `loading`, `processing` et `traning`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "class ProjectHooks:\n",
    "    @hook_impl\n",
    "    def register_pipelines(self) -> Dict[str, Pipeline]:\n",
    "        \"\"\"Register the project's pipeline.\n",
    "\n",
    "        Returns:\n",
    "            A mapping from a pipeline name to a ``Pipeline`` object.\n",
    "\n",
    "        \"\"\"\n",
    "        p_processing = processing_pipeline.create_pipeline()\n",
    "        p_training = training_pipeline.create_pipeline()\n",
    "        p_loading = loading_pipeline.create_pipeline()\n",
    "        return {\n",
    "            \"global\": Pipeline([p_loading, p_processing, p_training]),\n",
    "            \"loading\": p_loading,\n",
    "            \"processing\": p_processing,\n",
    "            \"training\": p_training,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex√©cutons le pipeline `global`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "kedro run --pipeline global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apr√®s quelques dizaines de secondes, le run devrait √™tre visible sous MLflow.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/model_registry2.png\" />\n",
    "\n",
    "N'oublions pas, puisque le test est concluant, de remettre `automl_max_evals` √† sa valeur d'origine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "break": "new"
   },
   "source": [
    "## Pipeline de d√©ploiement vers MLflow\n",
    "\n",
    "Pour l'instant, nous avons uniquement effectu√© du tracking. Bien que cela soit pratique pour historiser et garder une trace des diff√©rentes ex√©cutions des pipelines d'entra√Ænement, cela ne permet pas directement de g√©rer efficacement des **versions de mod√®les**.\n",
    "\n",
    "### Registre de mod√®les\n",
    "\n",
    "Le **Model Registry** (registre de mod√®les) est un composant de MLflow qui permet de g√©rer des versions de mod√®les de Machine Learning, en proposant √©galement des **stages** (√©tats).\n",
    "\n",
    "- Le tag **staging** correspond √† un mod√®le consid√©r√© comme pr√©-production.\n",
    "- Le tag **production** correspond √† un mod√®le qui serait en environnement de production.\n",
    "- Le tag **archived** pour les anciens mod√®les staging ou production archiv√©s.\n",
    "\n",
    "C'est un composant particuli√®rement utile pour g√©rer le cycle de vie des mod√®les, car le cycle staging, production et archive est couramment appliqu√© lorsque des mod√®les sont mis √† jour r√©guli√®rement. Sous MLflow, l'onglet Models permet d'afficher tous les mod√®les enregistr√©s.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/model_registry4.png\" />\n",
    "\n",
    "Retournons dans les exp√©riences et choisissons le dernier run que nous avons lanc√©. En cliquant sur `model` dans les artifacts, un bouton *Register Model* appara√Æt √† droite.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/model_registry5.png\" />\n",
    "\n",
    "En cliquant dessus, nous allons pouvoir ajouter manuellement le mod√®le au registre. Pour cela, nous devons cr√©er un nouveau mod√®le que l'on nommera `purchase_predict`.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/model_registry6.png\" />\n",
    "\n",
    "En retournant dans Models, nous voyons que la version 1 (le mod√®le que nous venons d'ajouter est bien pr√©sent).\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/model_registry7.png\" />\n",
    "\n",
    "En cliquant dessus, nous avons acc√®s √† plus de d√©tails sur les diff√©rentes versions pr√©sentes.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/model_registry8.png\" />\n",
    "\n",
    "Pour une version sp√©cifique, nous pouvons manuellement transitionner vers un √©tat de pr√©-production ou de production. L'objectif sera d'automatiser cette t√¢che pour automatiquement changer l'√©tat d'un mod√®le que l'on aura entra√Æn√©.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/model_registry9.png\" />\n",
    "\n",
    "### Pipeline de d√©ploiement\n",
    "\n",
    "Une fois l'entra√Ænement r√©alis√©, nous allons √©galement impl√©menter dans Kedro un pipeline qui va permettre de transitionner l'√©tat d'un mod√®le en staging ou production. Pour cela, nous allons cr√©er un pipepline `deployment` avec les trois fichiers `__init__.py`, `nodes.py` et `pipeline.py`. Le fichier `__init__.py` contiendra les m√™mes instructions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons cr√©er deux fonctions dans le fichier `nodes.py`.\n",
    "\n",
    "- La fonction `push_to_model_registry` va envoyer un mod√®le enregistr√© dans tracking vers le registre de mod√®le associ√©. Notons que pour cela, le mod√®le doit d√©j√† √™tre dans le tracking.\n",
    "- La fonction `stage_model` qui permet de transitionner un mod√®le du registre vers un √©tat staging ou production."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "\n",
    "def push_to_model_registry(registry_name: str, run_id: int):\n",
    "    \"\"\"\n",
    "    Pushes a model's version to the specified registry.\n",
    "    \"\"\"\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_SERVER\"))\n",
    "    result = mlflow.register_model(\n",
    "        \"runs:/{}/artifacts/model\".format(run_id), registry_name\n",
    "    )\n",
    "    return result.version\n",
    "\n",
    "\n",
    "def stage_model(registry_name: str, version: int):\n",
    "    \"\"\"\n",
    "    Stages a model version pushed to model registry.\n",
    "    \"\"\"\n",
    "    env = os.getenv(\"ENV\")\n",
    "    if env not in [\"staging\", \"production\"]:\n",
    "        return\n",
    "\n",
    "    client = MlflowClient()\n",
    "    client.transition_model_version_stage(\n",
    "        name=registry_name, version=int(version), stage=env[0].upper() + env[1:]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons besoin de configurations suppl√©mentaires, `registry_name`, pour r√©f√©rencer le nom du mod√®le dans le registre, que l'on ajoute au fichier `parameters.yml`, ainsi que la variable d'environnement `ENV` pour sp√©cifier si le projet Kedro est ex√©cut√© dans un environnement de pr√©-production ou de production. Modifions le fichier `.env`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "plaintext"
   },
   "source": [
    "ENV=staging\n",
    "# Remplacer par l'adresse IP de la VM contenant MLflow\n",
    "MLFLOW_SERVER=http://xx.xx.xx.xx/\n",
    "GOOGLE_APPLICATION_CREDENTIALS=\"conf/local/service-account.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De m√™me pour le fichier `parameters.yml`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "yaml"
   },
   "source": [
    "mlflow_model_registry: \"purchase_predict\" # Name of model registry of this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le param√®tre `run_id` sera lui fourni par le catalogue de donn√©es. √âcrivons le code du pipeline, qui ne pr√©sente aucune difficult√©."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from kedro.pipeline import Pipeline, node\n",
    "\n",
    "from .nodes import push_to_model_registry, stage_model\n",
    "\n",
    "def create_pipeline(**kwargs):\n",
    "    return Pipeline(\n",
    "        [\n",
    "            node(\n",
    "                push_to_model_registry,\n",
    "                [\"params:mlflow_model_registry\", \"mlflow_run_id\"],\n",
    "                \"mlflow_model_version\",\n",
    "            ),\n",
    "            node(\n",
    "                stage_model,\n",
    "                [\"params:mlflow_model_registry\", \"mlflow_model_version\"],\n",
    "                None,\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'√† renseigner ce pipeline dans `hooks.py`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from purchase_predict.pipelines.deployment import pipeline as deployment_pipeline\n",
    "\n",
    "class ProjectHooks:\n",
    "    @hook_impl\n",
    "    def register_pipelines(self) -> Dict[str, Pipeline]:\n",
    "        \"\"\"Register the project's pipeline.\n",
    "\n",
    "        Returns:\n",
    "            A mapping from a pipeline name to a ``Pipeline`` object.\n",
    "\n",
    "        \"\"\"\n",
    "        p_processing = processing_pipeline.create_pipeline()\n",
    "        p_training = training_pipeline.create_pipeline()\n",
    "        p_loading = loading_pipeline.create_pipeline()\n",
    "        p_deployment = deployment_pipeline.create_pipeline()\n",
    "        return {\n",
    "            \"global\": Pipeline([p_loading, p_processing, p_training, p_deployment]),\n",
    "            \"loading\": p_loading,\n",
    "            \"processing\": p_processing,\n",
    "            \"training\": p_training,\n",
    "            \"deployment\": p_deployment\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons le pipeline suivant.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/model_registry10.png\" />\n",
    "\n",
    "Lan√ßons une ex√©cution globale (en sp√©cifiant l√†-aussi le param√®tre `automl_max_evals` √† 1 pour acc√©lerer les calculs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "kedro run --pipeline global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque nous sommes en environnement staging, sur MLflow, le mod√®le enregistr√© doit avoir l'√©tat correspondant.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/model_registry12.png\" />\n",
    "\n",
    "Nous avons donc √† la fois la *Latest Version* √† 2, et c'est elle qui est actuellement en staging.\n",
    "\n",
    "L'int√©gralit√© des pipelines ont √©t√© r√©alis√©es, ce qui donne pour le pipeline `global` un ensemble assez important d'√©tapes.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/model_registry11.png\" />\n",
    "\n",
    "> ‚ùì En quoi envoyer le mod√®le vers le registre peut nous √™tre utile ?\n",
    "\n",
    "Ce qui va √™tre puissant, c'est que l'on sera capable de r√©cup√©rer, dans un projet diff√©rent que Kedro, le mod√®le le plus √† jour. Ainsi, on **d√©couple fortement** la phase d'exp√©rimentation/d'entra√Ænement du mod√®le et la phase de d√©ploiement, qui est une bonne pratique du Cloud.\n",
    "\n",
    "Comme toujours, il ne faut pas h√©siter √† r√©guli√®rement pousser son code vers le d√©p√¥t Git."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "git add .\n",
    "git commit -am \"Integrated all pipelines\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "console"
   },
   "source": [
    "black....................................................................Passed\n",
    "flake8...................................................................Passed\n",
    "[master 31d0824] Integrated all pipelines\n",
    " 20 files changed, 549 insertions(+), 16 deletions(-)\n",
    " create mode 100644 src/purchase_predict/pipelines/deployment/__init__.py\n",
    " create mode 100644 src/purchase_predict/pipelines/deployment/nodes.py\n",
    " create mode 100644 src/purchase_predict/pipelines/deployment/pipeline.py\n",
    " create mode 100644 src/purchase_predict/pipelines/training/__init__.py\n",
    " create mode 100644 src/requirements.in\n",
    " create mode 100644 src/tests/pipelines/loading/__init__.py\n",
    " create mode 100644 src/tests/pipelines/loading/conftest.py\n",
    " create mode 100644 src/tests/pipelines/loading/test_nodes.py\n",
    " create mode 100644 src/tests/pipelines/loading/test_pipeline.py\n",
    " create mode 100644 src/tests/pipelines/processing/__init__.py\n",
    " create mode 100644 src/tests/pipelines/processing/conftest.py\n",
    " create mode 100644 src/tests/pipelines/processing/test_nodes.py\n",
    " create mode 100644 src/tests/pipelines/processing/test_pipeline.py\n",
    "(venv) jovyan@jupyter-604a3b76-2d8f51-2d448a-2d8a79-2d0bb4e5cd11e4:~/purchase_predict$ git status\n",
    "On branch master\n",
    "nothing to commit, working tree clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons cr√©er une **nouvelle branche** appel√©e staging. Cela va nous permettre de produire un code en environnement de pr√©-production."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "git checkout -b staging\n",
    "git commit -am \"New staging branch\"\n",
    "git push google staging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On n'oubliera pas d'ajouter la cl√© SSH √† l'agent avec de pousser vers le d√©p√¥t."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "ssh-add ~/ssh/git_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur <a href=\"https://source.cloud.google.com/\" target=\"_blank\">Cloud Source</a>, la branche `staging` est d√©sormais visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "type": "ending"
   },
   "source": [
    "## ‚úîÔ∏è Conclusion\n",
    "\n",
    "Dor√©navant, le projet Kedro peut ex√©cuter le pipeline ML de A √† Z avec MLflow pour centraliser les diff√©rents mod√®les qui seront entra√Æn√©s.\n",
    "\n",
    "- Nous avons int√©gr√© le tracking MLflow sous Kedro.\n",
    "- Nous avons cr√©er un mod√®le packag√© sous MLflow.\n",
    "- Nous sommes capable de changer les √©tats des mod√®les de pr√©-production et de production automatiquement.\n",
    "\n",
    "> ‚û°Ô∏è Nous avons un mod√®le pr√™t √† √™tre utilis√©. Mais pour √™tre utilis√©, il faut le rendre accessible ... et la meilleure mani√®re de le faire, c'est de <b>construire une API</b>."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "√âditer les M√©ta-Donn√©es",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
