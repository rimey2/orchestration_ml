{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "type": "intro"
   },
   "source": [
    "Dans l'univers du d√©veloppement logiciel, les tests sont omni-pr√©sents. Ils permettent de v√©rifier que le logiciel ou l'application d√©velopp√©e adopte correctement le comportement attendu, ne produit pas de bugs ou s'int√®gre efficacement dans un environnement existant.\n",
    "\n",
    "Mais comment transposer tous ces tests de d√©veloppement logiciel au cas o√π l'on entra√Æne et fait intervenir des mod√®les de Machine Learning ?\n",
    "\n",
    "<blockquote><p>üôã <b>Ce que nous allons faire</b></p>\n",
    "<ul>\n",
    "    <li>Comprendre pourquoi il est important de tester son code et son mod√®le</li>\n",
    "    <li>Appliquer les tests usuels de d√©veloppement logiciel</li>\n",
    "</ul>\n",
    "</blockquote>\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/Rd6sn03ncIklmprvy6/giphy.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tester des applications\n",
    "\n",
    "Avant de rentrer dans le d√©tails des tests d'algorithmes de Machine Learning, d√©crivons tout d'abord les bonnes pratiques h√©rit√©es du d√©veloppement logiciel.\n",
    "\n",
    "### Tests logiciels\n",
    "\n",
    "Dans ce contexte, une suite de tests inclut habituellement trois composantes.\n",
    "\n",
    "- Les **tests unitaires**, o√π l'on s'assurer qu'une portion atomique du code fonctionne correctement (par exemple, une fonction). En r√®gle g√©n√©rale, ce sont des tests rapides et faciles √† mettre en place.\n",
    "- Les **tests de r√©gression**, o√π l'on doit s'assurer que le d√©veloppement d'une nouvelle fonctionnalit√© ne va pas faire survenir un bug d√©j√† rencontr√© par le pass√©.\n",
    "- Les **tests d'int√©gration**, o√π on cherche √† voir si la fonctionnalit√© d√©velopp√©e va √™tre correctement int√©gr√© dans l'application sans g√©n√©rer des erreurs dues √† son interaction avec d'autres composantes. Ces erreurs sont en pratique plus difficiles √† pr√©venir, d'o√π la difficult√© de construire des tests d'int√©gration efficaces.\n",
    "\n",
    "Dans les faits, les bonnes pratiques n√©cessitent de suivre plusieurs conventions. En travail collaboratif, notamment avec `git`, les r√®gles de base suivantes sont appliqu√©es.\n",
    "\n",
    "- Ne **jamais fusionner de branches** si les tests ne sont pas valides.\n",
    "- **Toujours √©crire des tests** pour de nouvelles fonctionnalit√©s.\n",
    "- Lorsque l'on corrige un bug, **toujours √©crire le test** et l'appliquer sur la correction.\n",
    "\n",
    "### Tests de mod√®les de Machine Learning\n",
    "\n",
    "Essayons maintenant de transposer ce que nous venons de voir pour tester les mod√®les de Machine Learning. Une fois un mod√®le de Machine Learning calibr√©, nousz souhaiterions obtenir un rapport d'√©valuation contenant les informations suivantes.\n",
    "\n",
    "- Performances avec des m√©triques d√©finies sur des sous-ensembles (`X_test` par exemple).\n",
    "- Graphes de validation : courbe PR, courbe ROC, densit√© des classes, courbe de calibration.\n",
    "- Audit du mod√®le avec des mod√®les d'interpr√©tabilit√© (PDP, valeurs de Shapley).\n",
    "- Sous-population o√π le mod√®le g√©n√®re des faux-positifs ou faux-n√©gatifs avec un fort degr√© de confiance.\n",
    "\n",
    "Par ailleurs, on y retrouve √©galement d'autres bonnes pratiques qui s'inscrivent toujours dans une logique de d√©marche de qualit√©.\n",
    "\n",
    "- **Toujours sauvegarder** les hyper-param√®tres, sous-√©chantillons utilis√©s et le mod√®le entra√Æn√©.\n",
    "- Mettre √† jour un environnement de production avec **un mod√®le aux meilleures performances** ou selon un seuil minimal.\n",
    "\n",
    "Face √† ces besoins de tester, nous pouvons voir que calculer des performances sur un sous-√©chantillon ou afficher des courbes n'est pas suffisant pour s'assurer que le mod√®le est ¬´ valide ¬ª. Pour les syst√®mes de Machine Learning, nous devrions effectuer deux m√©thodes en parall√®le.\n",
    "\n",
    "- **L'√©valuation de mod√®le**, o√π l'on calcule ses performances, audite son fonctionnement et affiche des courbes.\n",
    "- Le **test de mod√®le** o√π l'on d√©veloppe des tests explicites pour v√©rifier que le comportement du mod√®le est bien celui attendu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "break": "new"
   },
   "source": [
    "## Tests unitaires\n",
    "\n",
    "Commen√ßons par introduire les tests unitaires avec `pytest`. Il s'agit d'une librairie qui permet de **faciliter la mise en place et l'ex√©cution** des tests de code sous Python. Bien que les tests unitaires puissent √™tre r√©alis√©s *from scratch*, `pytest` am√©liore la productivit√© et apporte des fonctionnalit√©s tr√®s utiles.\n",
    "\n",
    "Testons la librairie sur le premier fichier suivant. Nous avons cod√© la fonction `argmax` qui cherche √† obtenir la position du plus grand √©l√©ment d'une liste. Nous codons √©galement la fonction `test_argmax` qui va tester unitairement la fonction `argmax` sur plusieurs exemples : cela refl√®te du comportement attendu de la fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0aaa75f81cffb2d6"
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/pytest_1.py\n",
    "def argmax(liste):\n",
    "    if len(liste) == 0:\n",
    "        return None\n",
    "    \n",
    "    idx_max = 0\n",
    "    value_max = liste[0]\n",
    "    for i, x in enumerate(liste):\n",
    "        if x > value_max:\n",
    "            value_max = x\n",
    "            idx_max = i\n",
    "    return idx_max\n",
    "\n",
    "def test_argmax():\n",
    "    assert argmax([5, 8, 2, 9, 6, 3]) == 3\n",
    "    assert argmax([7]) == 0\n",
    "    assert argmax([]) == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex√©cutons le code avec `pytest` en sp√©cifiant le chemin d'acc√®s au fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4b314d2af0315b00"
   },
   "outputs": [],
   "source": [
    "!pytest /tmp/pytest_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ex√©cutant cette commande, `pytest` effectue une **d√©couverte automatique** des tests.\n",
    "\n",
    "- Il va d'abord rechercher tous les fichiers dont le nom commence par `test*` si on lui fournit un dossier.\n",
    "- Pour chaque classe/fonction du fichier, si l'objet commence par `test*`, alors ce dernier sera instanci√© (dans le cas d'une fonction) et les fonctions seront ex√©cut√©es (pour les deux).\n",
    "\n",
    "Cette d√©couverte des tests permet de simplifier la mise en place des tests : plus besoin de sp√©cifier tous les tests dans un fichier, qui lui-m√™me effectue des importations. Nous pouvons imaginer que pour chaque *module*, il y ait un fichier `test.py` qui regroupe tous les tests unitaires li√©s √† ce module. De mani√®re g√©n√©rale, il est plus appropri√© de cr√©er un fichier sp√©cifique pour les tests unitaires plut√¥t que de les ins√©rer dans le code qui fournit la logique √† l'application.\n",
    "\n",
    "C'est de cette mani√®re que `pytest` ex√©cute naturellement la fonction `test_argmax` sans avoir eu besoin de la sp√©cifier comme argument. Dans certains cas, nous pouvons √™tre amen√© √† √©viter volontairement l'ex√©cution d'une fonction. Dans ce cas, il suffit d'ajouter le d√©corateur `pytest.mark.skip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1f2cc277d7ac47dd"
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/pytest_1.py\n",
    "import pytest\n",
    "\n",
    "def argmax(liste):\n",
    "    if len(liste) == 0:\n",
    "        return None\n",
    "    \n",
    "    idx_max = 0\n",
    "    value_max = liste[0]\n",
    "    for i, x in enumerate(liste):\n",
    "        if x > value_max:\n",
    "            value_max = x\n",
    "            idx_max = i\n",
    "    return idx_max\n",
    "\n",
    "@pytest.mark.skip\n",
    "def test_argmax():\n",
    "    assert argmax([5, 8, 2, 9, 6, 3]) == 3\n",
    "    assert argmax([7]) == 0\n",
    "    assert argmax([]) == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2726a1c9c8788ae3"
   },
   "outputs": [],
   "source": [
    "!pytest /tmp/pytest_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir, 100% des tests ont r√©ussi car le seul test pr√©sent a √©t√© ignor√© (*skipped*). Voyons maintenant un autre fichier Python dont le test unitaire va volontairement g√©n√©rer une erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "b42e9e449064882c"
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/pytest_2.py\n",
    "def argmin(liste):\n",
    "    if len(liste) == 0:\n",
    "        return None\n",
    "    \n",
    "    idx_min = 0\n",
    "    value_min = liste[0]\n",
    "    for i, x in enumerate(liste):\n",
    "        if x < value_min:\n",
    "            value_min = x\n",
    "            idx_min = i + 1\n",
    "    return idx_min\n",
    "\n",
    "def test_argmin():\n",
    "    assert argmin([5, 8, 2, 9, 6, 3]) == 2\n",
    "    assert argmin([7]) == 0\n",
    "    assert argmin([]) == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "36774868e28d4d28"
   },
   "outputs": [],
   "source": [
    "!pytest /tmp/pytest_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'apr√®s la sortie g√©n√©r√©e par `pytest`, les tests du fichier `/tmp/pytest_2.py` ont √©chou√©s. Si l'on regarde en d√©taille l'ex√©cution de `test_argmin`, nous avons un `assert 3 == 2`, ce qui signifie que notre test unitaire a √©chou√©. Corrigeons la fonction `argmin` et ajoutons la fonction `argmax` avec son test unitaire associ√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1bddca2f7bb2a052"
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/pytest_2.py\n",
    "def argmin(liste):\n",
    "    if len(liste) == 0:\n",
    "        return None\n",
    "    \n",
    "    idx_min = 0\n",
    "    value_min = liste[0]\n",
    "    for i, x in enumerate(liste):\n",
    "        if x < value_min:\n",
    "            value_min = x\n",
    "            idx_min = i\n",
    "    return idx_min\n",
    "\n",
    "def argmax(liste):\n",
    "    if len(liste) == 0:\n",
    "        return None\n",
    "    \n",
    "    idx_max = 0\n",
    "    value_max = liste[0]\n",
    "    for i, x in enumerate(liste):\n",
    "        if x > value_max:\n",
    "            value_max = x\n",
    "            idx_max = i\n",
    "    return idx_max\n",
    "\n",
    "def test_argmin():\n",
    "    assert argmin([5, 8, 2, 9, 6, 3]) == 2\n",
    "    assert argmin([7]) == 0\n",
    "    assert argmin([]) == None\n",
    "    \n",
    "def test_argmax():\n",
    "    assert argmax([5, 8, 2, 9, 6, 3]) == 3\n",
    "    assert argmax([7]) == 0\n",
    "    assert argmax([]) == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eb410ca9a4c94ebd"
   },
   "outputs": [],
   "source": [
    "!pytest /tmp/pytest_2.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le param√®tre `-v` permet d'afficher plus de d√©tails concernant les tests. Puisque deux fonctions sont nomm√©es `test*`, il y a deux tests effectu√©s par `pytest`. Cette option permet d'obtenir un d√©tail pour chaque test cod√©, simplifiant ensuite le d√©boggage de l'application. \n",
    "\n",
    "En pratique, les tests unitaires doivent √™tre ex√©cut√©s une fois les donn√©es envoy√©s vers le d√©p√¥t Git. En revanche, il est d√©conseill√© de les ex√©cuter lors du pre-commit, car ce dernier doit √™tre rapide. Les tests unitaires, notamment ceux incluant des tests pour les mod√®les, peuvent prendre du temps ce qui n'est pas conseill√© pour les pre-commits.\n",
    "\n",
    "### Les fixtures\n",
    "\n",
    "Imaginons que l'on souhaite utiliser des donn√©es/param√®tres uniquement pour les tests unitaires. Si l'on regarde bien, les deux fonctions `test_argmin` et `test_argmax` utilisent les m√™mes listes pour tester les deux fonctions. Nous pourrions tout √† fait d√©finir des catalogues de r√©f√©rence pour les tests unitaires qui seront utilis√©s √† chaque fois. C'est √† cela que servent **les fixtures**.\n",
    "\n",
    "Regardons le code suivant qui n'utilise pas de fixture. Nous allons simplement cr√©er une liste `test_data` qui sera utilis√©e par les deux fonctions de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "911e4730a0c0c146"
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/pytest_2.py\n",
    "\n",
    "# Pas bien !\n",
    "test_data = [5, 8, 2, 9, 6, 3]\n",
    "\n",
    "def argmin(liste):\n",
    "    if len(liste) == 0:\n",
    "        return None\n",
    "    \n",
    "    idx_min = 0\n",
    "    value_min = liste[0]\n",
    "    for i, x in enumerate(liste):\n",
    "        if x < value_min:\n",
    "            value_min = x\n",
    "            idx_min = i\n",
    "    return idx_min\n",
    "\n",
    "def argmax(liste):\n",
    "    if len(liste) == 0:\n",
    "        return None\n",
    "    \n",
    "    idx_max = 0\n",
    "    value_max = liste[0]\n",
    "    for i, x in enumerate(liste):\n",
    "        if x > value_max:\n",
    "            value_max = x\n",
    "            idx_max = i\n",
    "    return idx_max\n",
    "\n",
    "def test_argmin():\n",
    "    assert argmin(test_data) == 2\n",
    "    \n",
    "def test_argmax():\n",
    "    assert argmax(test_data) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "59efd5f38522c5f1"
   },
   "outputs": [],
   "source": [
    "!pytest /tmp/pytest_2.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que le test ait fonctionn√©, cela n'est pas une bonne pratique, car nous allons obligatoirement d√©finir cette variable globale en m√©moire √† chaque ex√©cution du code, alors qu'elle n'est utilis√©e que pour les tests unitaires. Dans ce cas de figure, il est pr√©f√©rable de cr√©er des fixtures.\n",
    "\n",
    "Les fixtures d√©finissent un environnement dans lequel nous allons pouvoir tester notre code. Dans beaucoup de situations, il nous faut initialiser certaines variables avant de lancer les tests unitaires. Les fixtures sous `pytest` sont des fonctions qui sont utilis√©s comme **param√®tres** des fonctions de tests unitaires.\n",
    "\n",
    "Regardons le code suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5b2b0ed56e8ebca4"
   },
   "outputs": [],
   "source": [
    "%%writefile /tmp/pytest_2.py\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def test_data():\n",
    "    return [5, 8, 2, 9, 6, 3]\n",
    "\n",
    "def argmin(liste):\n",
    "    if len(liste) == 0:\n",
    "        return None\n",
    "    \n",
    "    idx_min = 0\n",
    "    value_min = liste[0]\n",
    "    for i, x in enumerate(liste):\n",
    "        if x < value_min:\n",
    "            value_min = x\n",
    "            idx_min = i\n",
    "    return idx_min\n",
    "\n",
    "def argmax(liste):\n",
    "    if len(liste) == 0:\n",
    "        return None\n",
    "    \n",
    "    idx_max = 0\n",
    "    value_max = liste[0]\n",
    "    for i, x in enumerate(liste):\n",
    "        if x > value_max:\n",
    "            value_max = x\n",
    "            idx_max = i\n",
    "    return idx_max\n",
    "\n",
    "def test_argmin(test_data):\n",
    "    assert argmin(test_data) == 2\n",
    "    \n",
    "def test_argmax(test_data):\n",
    "    assert argmax(test_data) == 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous d√©finissons la fonction `test_data` comme fixture √† l'aide du d√©corateur de fonctions `@pytest.fixture`. Cette fonction va renvoyer une liste qui correspond √† la liste de r√©f√©rence pour tester les deux fonctions. Ensuite, dans les fonctions de tests unitaires, nous allons r√©cup√©rer comme param√®tre cette m√™me fonction `test_data`. Mais attention : lorsque l'on ex√©cutera `pytest`, ce dernier va automatiquement remplacer le param√®tre `test_data` (qui est suppos√© √™tre une fonction car fixture) par le r√©sultat de cette fonction.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/tests4.png\" />\n",
    "\n",
    "Ainsi, √† chaque ex√©cution de `pytest`, ce sera en r√©alit√© `test_data()` qui sera pass√© comme param√®tre pour les fonctions `test_argmin` et `test_argmax` (et non la fonction `test_data` elle-m√™me). Cette m√©thode permet d'instancier plus efficacement les initialisations pour les tests, sans compromettre le reste du code qui lui n'aura pas besoin des tests dans un environnement de production.\n",
    "\n",
    "Ex√©cutons maintenant `pytest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "47a6cd6781b5300d"
   },
   "outputs": [],
   "source": [
    "!pytest /tmp/pytest_2.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout a correctement fonctionn√©. L'int√©r√™t de ce syst√®me est de pouvoir ensuite centraliser l'initialisation des variables et des donn√©es pour les tests, √©vitant ainsi les duplicata de codes que l'on conna√Æt d√©j√† bien hors des tests.\n",
    "\n",
    "Maintenant que nous avons vu les points essentiels de `pytest`, nous pouvons dor√©navant int√©grer les tests unitaires dans notre projet Kedro. Et un avantage non n√©gligeable est que Kedro supporte nativement `pytest` pour les tests unitaires : il dispose m√™me de la commande `kedro test`. üôÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "break": "new"
   },
   "source": [
    "## Int√©gration des tests unitaires dans Kedro\n",
    "\n",
    "Int√©grons les tests unitaires et du mod√®le dans notre projet Kedro. En regardant la structure du projet, nous pouvons observer le dossier `src/tests` qui contient le fichier `test_run.py`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "plaintext"
   },
   "source": [
    "This module contains an example test.\n",
    "\n",
    "Tests should be placed in ``src/tests``, in modules that mirror your\n",
    "project's structure, and in files named test_*.py. They are simply functions\n",
    "named ``test_*`` which test a unit of logic.\n",
    "\n",
    "To run the tests, run ``kedro test`` from the project root directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ex√©cuter proprement les tests avec Kedro, il faut que la structure des fichiers des tests soit identique √† celle utilis√©e dans `src/purchase_predict`. Nous devons donc cr√©er deux dossiers `loading`, `training` et `processing` dans `src/tests/pipelines` pour r√©pliquer l'architecture √† l'identique.\n",
    "\n",
    "Commen√ßons par le dossier `loading` qui charge les fichiers CSV depuis Cloud Storage. Au pr√©alable, nous allons installer les d√©pendances de Kedro pour effectuer les tests unitaires (qui contient `pytest` notamment)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "pip install src/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests sur les nodes\n",
    "\n",
    "Avant de d√©velopper nos tests unitaires, cr√©ons sur le bucket Cloud Storage des **donn√©es de tests**. Dans le dossier `primary/` du bucket, nous allons cr√©er un dossier `data-test.csv/`.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/tests1.png\" />\n",
    "\n",
    "Ensuite, pour alimenter ce dossier, nous allons copier deux fichiers CSV d√©j√† pr√©sents dans `data.csv/` vers `data-test.csv/`.\n",
    "\n",
    "<img src=\"https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/training/ml_engineer_facebook/img/tests2.png\" />\n",
    "\n",
    "Habituellement, avec Kedro, nous pouvons effectuer deux s√©ries de tests.\n",
    "\n",
    "- Les tests sur les nodes et les fonctions qu'utilisent les nodes. Par exemple, pour l'entra√Ænement, le fichier `nodes.py` contient des fonctions qui ne sont pas directement utilis√©s par les nodes mais qui sont appel√©s par les fonctions des nodes.\n",
    "- Les tests sur les pipelines, permettant de les tester en fonction de plusieurs formats d'entr√©e ou sous forme d'ex√©cution partielles.\n",
    "\n",
    "Cr√©ons tout d'abord le fichier `test_nodes.py`. Dans le pipeline `loading`, seule la fonction `load_csv_from_bucket` est pr√©sente : nous allons uniquement tester cette derni√®re."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "from purchase_predict.pipelines.loading.nodes import load_csv_from_bucket\n",
    "\n",
    "def test_load_csv_from_bucket(project_id, primary_folder):\n",
    "    df = load_csv_from_bucket(project_id, primary_folder)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous d√©finissons la fonction `test_load_csv_from_bucket` avec les m√™mes param√®tres que la fonction `load_csv_from_bucket`.\n",
    "\n",
    "> ‚ùì Mais nous n'avons pas d√©fini les fixtures ici ?\n",
    "\n",
    "En effet, il faudrait que les param√®tres `project_id` et `primary_folder` soient des fixtures avec des fonctions de m√™me nom. Or, ici, nous n'en avons pas cr√©√©e. Il y a une raison √† cela : plus tard, nous allons √©galement cr√©er un fichier de test pour le pipeline. Pour √©viter des redondances de d√©finitions de fixtures, nous allons d√©finir les fixtures dans un fichier sp√©cifique, qui derri√®re sera automatiquement ex√©cut√© par `pytest`.\n",
    "\n",
    "D'apr√®s <a href=\"https://docs.pytest.org/en/stable/fixture.html#conftest-py-sharing-fixture-functions\" target=\"_blank\">la documentation</a> de `pytest` sur les fixtures, nous pouvons les centraliser dans un fichier nomm√© `conftest.py` qui sera automatiquement ex√©cut√© avant les tests unitaires. Nous en cr√©ons un dans le dossier `loading`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def project_id():\n",
    "    return \"<PROJECT_GCP>\"\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def primary_folder():\n",
    "    return \"<NOM_DU_BUCKET>/primary/data-test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'argument `scope=\"module\"` permet de sp√©cifier que les fixtures seront accessibles √† l'int√©rieur de `purchase_predict`. Il ne reste plus qu'√† lancer les test avec Kedro."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "kedro test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "console"
   },
   "source": [
    "========================= test session starts =========================\n",
    "platform linux -- Python 3.8.5, pytest-6.1.2, py-1.10.0, pluggy-0.13.1\n",
    "rootdir: /home/jovyan/purchase_predict, configfile: pyproject.toml\n",
    "plugins: mock-1.13.0, cov-2.11.0\n",
    "collected 2 items                                                                                                 \n",
    "\n",
    "src/tests/test_run.py .                        [ 50%]\n",
    "src/tests/pipelines/loading/test_nodes.py .    [100%]\n",
    "\n",
    "========================== 2 passed in 1.94s =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au tout d√©but, `pytest` ex√©cute le test sur `test_run.py`, qui montre un exemple de test unitaire avec Kedro. Ensuite, il ex√©cute le seul autre fichier de test pr√©sent `test_nodes.py`. Puisqu'il n'y a aucun probl√®me, cela signifie que le code n'a pas g√©n√©r√© d'erreurs et que, en th√©orie, nous avons correctement r√©ussi √† impl√©menter la fonction de test avec Kedro. C'est alors que nous pouvons rajouter des tests et des conditions dans la fonction."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "def test_load_csv_from_bucket(project_id, primary_folder):\n",
    "    df = load_csv_from_bucket(project_id, primary_folder)\n",
    "    assert type(df) == pd.DataFrame\n",
    "    assert df.shape[1] == 16\n",
    "    assert \"purchased\" in df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests sur les pipelines\n",
    "\n",
    "En plus de tests unitaires sur les nodes, il est √©galement possible d'effectuer des tests unitaires sur les pipelines. Cela permet, par exemple, de s'assurer du bon d√©roulement du pipeline en fonction de plusieurs situations (donn√©es incompl√®tes ou manquantes, mauvaise configuration de param√®tres). En respectant le m√™me principe que pour les nodes, nous allons cr√©er le fichier `test_pipeline.py`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from kedro.runner import SequentialRunner\n",
    "\n",
    "from purchase_predict.pipelines.loading.pipeline import create_pipeline\n",
    "\n",
    "def test_pipeline(catalog_test):\n",
    "    runner = SequentialRunner()\n",
    "    pipeline = create_pipeline()\n",
    "    pipeline_output = runner.run(pipeline, catalog_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous r√©cup√©rons la fonction `create_pipeline` permettant de cr√©er le pipeline que nous souhaitons tester. Dans le test unitaire, nous instancions un `SequentialRunner`, qui ex√©cutera le pipeline de mani√®re s√©quentielle. Ensuite, nous cr√©ons une instance du pipeline et enfin nous ex√©cuter ce dernier. Remarquons la variable `catalog_test` : il s'agit d'un catalogue de donn√©es sp√©cifiquement cr√©e pour le test. Plut√¥t que d'utiliser celui par d√©faut dans le fichier `catalog.yml`, nous allons pouvoir sp√©cifier des donn√©es propres aux tests qui ne va pas perturber le catalogue d√©j√† pr√©sent.\n",
    "\n",
    "Le catalogue de donn√©es repr√©sente lui aussi une fixture que nous rajoutons dans `conftest.py`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "import pytest\n",
    "\n",
    "from kedro.io import DataCatalog, MemoryDataSet\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def project_id():\n",
    "    return \"<PROJECT_GCP>\"\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def primary_folder():\n",
    "    return \"<NOM_DU_BUCKET>/primary/data-test.csv\"\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def catalog_test(project_id, primary_folder):\n",
    "    catalog = DataCatalog({\n",
    "        \"params:gcp_project_id\": MemoryDataSet(project_id),\n",
    "        \"params:gcs_primary_folder\": MemoryDataSet(primary_folder)\n",
    "    })\n",
    "    return catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction retourne un `DataCatalog` qui sera envoy√© en entr√©e au pipeline.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Il faut respecter les noms des variables sp√©cifi√©s dans le pipeline. \n",
    "</div>\n",
    "\n",
    "Pour rappel, le pipeline `loading` √©tait d√©fini de la mani√®re suivante."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "def create_pipeline(**kwargs):\n",
    "    return Pipeline(\n",
    "        [\n",
    "            node(\n",
    "                load_csv_from_bucket,\n",
    "                [\"params:gcp_project_id\", \"params:gcs_primary_folder\"],\n",
    "                \"primary\",\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L√†-aussi, `pytest` remplacera `catalog_test` par la fixture associ√©e et permettra d'initialiser correctement l'environnement de test."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "kedro test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "console"
   },
   "source": [
    "========================= test session starts =========================\n",
    "platform linux -- Python 3.8.5, pytest-6.1.2, py-1.10.0, pluggy-0.13.1\n",
    "rootdir: /home/jovyan/purchase_predict, configfile: pyproject.toml\n",
    "plugins: mock-1.13.0, cov-2.11.0\n",
    "collected 2 items                                                                                                 \n",
    "\n",
    "src/tests/test_run.py .                          [ 33%]\n",
    "src/tests/pipelines/loading/test_nodes.py .      [ 66%]\n",
    "src/tests/pipelines/loading/test_pipeline.py .   [100%]\n",
    "\n",
    "========================== 3 passed in 2.17s =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pipeline a √©t√© ex√©cut√© sans probl√®me. Nous pouvons l√†-aussi r√©diger des tests pour le pipeline, qui en soit seront quasi-identiques √† ceux du node car ce pipeline ne contient qu'un seul node et ce dernier n'appelle pas d'autres fonctions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "def test_pipeline(catalog_test):\n",
    "    runner = SequentialRunner()\n",
    "    pipeline = create_pipeline()\n",
    "    pipeline_output = runner.run(pipeline, catalog_test)\n",
    "    df = pipeline_output[\"primary\"]\n",
    "    assert type(df) == pd.DataFrame\n",
    "    assert df.shape[1] == 16\n",
    "    assert \"purchased\" in df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "type": "ending"
   },
   "source": [
    "## ‚úîÔ∏è Conclusion\n",
    "\n",
    "Peut-√™tre il s'agit de ton premier test unitaire avec Python : dans tous les cas, tu sais maintenant en √©crire, et c'est une tr√®s bonne pratique !\n",
    "\n",
    "- Nous avons vu pourquoi les tests logiciels √©taient indispensables.\n",
    "- Nous avons r√©dig√© plusieurs tests unitaires pour le pipeline de collecte des donn√©es.\n",
    "\n",
    "> ‚û°Ô∏è Il nous reste maintenant √† d√©finir et r√©diger les <b>tests sur le mod√®le de Machine Learning</b>."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "√âditer les M√©ta-Donn√©es",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
