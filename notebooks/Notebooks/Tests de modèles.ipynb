{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "type": "intro"
   },
   "source": [
    "Pr√©c√©demment, nous avons r√©alis√© des tests unitaires sur notre projet Kedro. Les tests unitaires sont rapides √† mettre en place et permettent de v√©rifier ind√©pendamment chaque portion de code.\n",
    "\n",
    "Mais lorsqu'il s'agit de mod√®les de Machine Learning, √† quoi correspondent exactement les tests ? Il n'est pas possible de r√©diger des tests unitaires de la m√™me fa√ßon, car un mod√®le fournit justement une pr√©diction, qui n'est a priori pas connu √† l'avance. C'est pourquoi les Data Scientists et ML Engineers ont imagin√© des m√©thodes de test √† r√©aliser sur les mod√®les.\n",
    "\n",
    "<blockquote><p>üôã <b>Ce que nous allons faire</b></p>\n",
    "<ul>\n",
    "    <li>Formaliser les diff√©rents tests qui peuvent survenir</li>\n",
    "    <li>R√©diger le tests dans le projet Kedro</li>\n",
    "</ul>\n",
    "</blockquote>\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/lQ0VQmLuLH7lS/giphy.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de mod√®les\n",
    "\n",
    "Lorsque l'on fait r√©f√©rence aux tests de mod√®les, on cherche √† v√©rifier le bon fonctionnement et comportement du mod√®le. Pour cela, nous distinguons deux classes de tests de mod√®les.\n",
    "\n",
    "- Les **tests pr√©-entra√Ænement**, qui nous permettent d'identifier des erreurs ou incoh√©rences avant m√™me l'entra√Ænement du mod√®le.\n",
    "- Les **tests post-entra√Ænement**, qui vont utiliser le mod√®le entra√Æn√© et inspecter le comportement de ce dernier par rapport √† des sc√©narios de r√©f√©rence que l'on d√©cide en amont.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Il n'y a pas de <i>meilleur test possible</i>. Chaque test doit √™tre d√©fini en fonction du contexte, du cas d'application et surtout de l'importance que l'on accorde aux d√©cisions du mod√®le.\n",
    "</div>\n",
    "\n",
    "## Les tests pr√©-entra√Ænement\n",
    "\n",
    "Les tests pr√©-entra√Ænement permettent d'√©viter de se lancer dans l'entra√Ænement d'un mod√®le si certains crit√®res ne sont pas respect√©s. Tr√®s souvent, ces crit√®res portent sur les donn√©es et les tests, bien que rapide √† mettre en place, permettent d√©j√† de soulever certains points. Parmi les tests qui peuvent √™tre r√©alis√©s avant l'entra√Ænement, nous retrouvons surtout de tests de coh√©rence des donn√©es.\n",
    "\n",
    "- Taille du jeu de donn√©es.\n",
    "- Format de la variable r√©ponse.\n",
    "- Proportion des classes dans la classification binaire.\n",
    "- Repr√©sentativit√© de l'√©chantillon par rapport √† la population d'√©tude.\n",
    "\n",
    "En soit, il s'agit de tests qui peuvent √™tre r√©dig√©s au m√™me titre que les tests unitaires pr√©c√©dents. D√©finissons les tests du pipeline `processing`. Ce pipeline est compos√© de deux nodes, chacun appelant une fonction.\n",
    "\n",
    "- `encode_features`, qui va encoder num√©riquement les variables de `primary`.\n",
    "- `split_dataset`, qui va s√©parer le jeu de donn√©es en une base d'apprentissage et une base de test.\n",
    "\n",
    "Commen√ßons par la fonction `encode_features` : elle s'attend √† recevoir un DataFrame nomm√© `dataset`. Il y a plusieurs choses que nous devons v√©rifier √† l'issue de cette fonction.\n",
    "\n",
    "- La colonne `purchased` est-elle toujours *intacte* dans le sens o√π elle n'est constitu√©e que de $0$ et de $1$ ?\n",
    "- Toutes les colonnes sont-elles num√©riques ?\n",
    "- Avons-nous suffisamment d'observations pour entra√Æner le mod√®le ?\n",
    "- Les proportions de classes positives et n√©gatives sont-elles au moins sup√©rieures √† un seuil ?\n",
    "\n",
    "Dans le dossier de test, cr√©ons le dossier `processing` avec le fichier `conftest.py`. Nous allons d√©finir un catalogue de donn√©es pour ce test."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "import pytest\n",
    "\n",
    "from purchase_predict.pipelines.loading.nodes import load_csv_from_bucket\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def project_id():\n",
    "    return \"PROJET_GCP\"  # TODO : Penser √† changer le nom du projet GCP\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def primary_folder():\n",
    "    return \"FICHIER_CSV\"  # TODO : Penser √† changer l'URL gs:// du fichier CSV\n",
    "    \n",
    "@pytest.fixture(scope=\"module\")\n",
    "def dataset_not_encoded(project_id, primary_folder):\n",
    "    return load_csv_from_bucket(project_id, primary_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous r√©-utilisons ici les deux fixtures `project_id` et `primary_folder` d√©j√† pr√©sentes dans les tests du pipeline `loading`. Nous utilisons la fonction `load_csv_from_bucket` pour r√©cup√©rer le jeu de donn√©es de test depuis Cloud Storage afin d'utiliser une version non alt√©r√©e qui serait enregistr√©e en local. En pratique, la fonction `load_csv_from_bucket` aura d√©j√† √©t√© test√©e au pr√©alable par `pytest`, nous pouvons donc l'utiliser ici pour charger les donn√©es.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Il faut √©viter de cr√©er des d√©pendances entre les tests : c'est pour cela que l'on red√©fini ici les fixtures sans les importer depuis <code>loading/conftest.py</code>.\n",
    "</div>\n",
    "\n",
    "Cr√©ons ensuite comme nous l'avions fait le fichier `test_nodes.py` dans le dossier `processing`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "from purchase_predict.pipelines.processing.nodes import encode_features\n",
    "\n",
    "def test_encode_features(dataset_not_encoded):\n",
    "    df = encode_features(dataset_not_encoded)[\"features\"]\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `test_encode_features` va tester `encode_features` √† partir de la fixture `dataset_not_encoded` que nous venons de d√©finir dans `conftest.py`. Ex√©cutons les tests avec Kedro."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "kedro test src/tests/pipelines/processing/ -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec l'argument `src/tests/pipelines/processing/`, on pr√©cise √† Kedro d'ex√©cuter les tests de mani√®re r√©cursive uniquement dans ce dossier.\n",
    "\n",
    "Si tout s'est bien pass√©, alors nos fixtures sont correctement en place et nous pouvons int√©grer les **tests pr√©-entra√Ænement**."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "BALANCE_THRESHOLD = 0.1\n",
    "MIN_SAMPLES = 5000\n",
    "\n",
    "def test_encode_features(dataset_not_encoded):\n",
    "    df = encode_features(dataset_not_encoded)[\"features\"]\n",
    "    # Checking column 'purchased' that all values are either 0 or 1\n",
    "    assert df['purchased'].isin([0, 1]).all()\n",
    "    # Checking that all columns are numeric\n",
    "    for col in df.columns:\n",
    "        assert pd.api.types.is_numeric_dtype(df.dtypes[col])\n",
    "    # Checking that we have enough samples\n",
    "    assert df.shape[0] > MIN_SAMPLES\n",
    "    # Checking that classes have at least BALANCE_THRESHOLD percent of data\n",
    "    assert (df['purchased'].value_counts() / df.shape[0] > BALANCE_THRESHOLD).all()\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les constantes `BALANCE_THRESHOLD` et `MIN_SAMPLES` vont bien s√ªr d√©pendrent du contexte. Dans certains situations, `BALANCE_THRESHOLD` devra √™tre amen√© √† $1\\%$ ou `MIN_SAMPLES` devra √™tre bien plus cons√©quent.\n",
    "\n",
    "√âcrivons maintenant les tests pour la fonction `split_dataset`. Il nous faut rajouter deux fixtures dans le fichier `conftest.py` : une pour le jeu de donn√©es encod√© (apr√®s application de `encode_features`) et une pour le ratio de test."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from purchase_predict.pipelines.processing.nodes import encode_features\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def test_ratio():\n",
    "    return 0.3\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def dataset_encoded(dataset_not_encoded):\n",
    "    return encode_features(dataset_not_encoded)[\"features\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la m√™me mani√®re, nous ajoutons le test `test_split_dataset` au fichier `test_nodes.py`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_split_dataset(dataset_encoded, test_ratio):\n",
    "    X_train, y_train, X_test, y_test = split_dataset(dataset_encoded, test_ratio).values()\n",
    "    # Checks both sets size\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "    assert X_test.shape[0] == y_test.shape[0]\n",
    "    assert X_train.shape[0] + X_test.shape[0] == dataset_encoded.shape[0]\n",
    "    # Note that train_test_split of scikit-learn use np.ceil for test split\n",
    "    # https://github.com/scikit-learn/scikit-learn/blob/42aff4e2edd8e8887478f6ff1628f27de97be6a3/sklearn/model_selection/_split.py#L1797\n",
    "    assert np.ceil(dataset_encoded.shape[0] * test_ratio) == X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lan√ßons les tests."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "kedro test src/tests/pipelines/processing/ -s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "========================= test session starts =========================\n",
    "platform linux -- Python 3.8.5, pytest-6.1.2, py-1.10.0, pluggy-0.13.1\n",
    "rootdir: /home/jovyan/purchase_predict, configfile: pyproject.toml\n",
    "plugins: mock-1.13.0, cov-2.11.0\n",
    "collected 2 items\n",
    "\n",
    "src/tests/pipelines/processing/test_nodes.py ..\n",
    "\n",
    "========================== 2 passed in 0.65s =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Les deux points apr√®s le chemin d'acc√®s au fichier signifie que deux fonctions de tests ont √©t√© correctement ex√©cut√©es. En cas d'erreur nous aurions eu un <code>E</code> √† la place et en cas de <i>skipping</i>, nous aurions eu <code>S</code>.\n",
    "</div>\n",
    "\n",
    "Il ne reste plus qu'√† tester le pipeline `processing` en entier, qui fait appel aux deux fonctions. √Ä noter que dans ce cas, il n'y a pas besoin de tester le jeu de donn√©es interm√©diaire car les tests unitaires sont suppos√©s d√©j√† valides. Pour rappel, le pipeline `processing` est constitu√© de deux nodes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "def create_pipeline(**kwargs):\n",
    "    return Pipeline(\n",
    "        [\n",
    "            node(\n",
    "                encode_features,\n",
    "                \"primary\",\n",
    "                \"dataset\",\n",
    "            ),\n",
    "            node(\n",
    "                split_dataset,\n",
    "                [\"dataset\", \"params:test_ratio\"],\n",
    "                dict(\n",
    "                    X_train=\"X_train\",\n",
    "                    y_train=\"y_train\",\n",
    "                    X_test=\"X_test\",\n",
    "                    y_test=\"y_test\",\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cr√©ons un catalogue de test pour `processing` dans le fichier `conftest.py`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from kedro.io import DataCatalog, MemoryDataSet\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def catalog_test(dataset_not_encoded, test_ratio):\n",
    "    catalog = DataCatalog({\n",
    "        \"primary\": MemoryDataSet(dataset_not_encoded),\n",
    "        \"params:test_ratio\": MemoryDataSet(test_ratio)\n",
    "    })\n",
    "    return catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour le pipeline pr√©c√©dent, nous nous basons sur les fixtures `dataset_not_encoded` et `test_ratio` pour cr√©er le catalogue de donn√©es.\n",
    "\n",
    "> ‚ùì Pourquoi n'avons-nous pas cr√©e un dataset <code>dataset</code> ?\n",
    "\n",
    "Le catalogue de donn√©es de test est identique √† celui utilis√© hors environnement de test, √† la diff√©rence que l'on sp√©cifie nous-m√™me les diff√©rentes entr√©es. Ainsi, remarquons que le premier node va produire en sortie `dataset`. En ex√©cutant le pipeline avec le catalogue de test, `dataset` sera alors stock√© dans le catalogue en tant que `MemoryDataSet` : il sera donc utilisable par le prochain node du pipeline.\n",
    "\n",
    "Le contenu de `test_pipeline.py` s'√©crit directement."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "python"
   },
   "source": [
    "from kedro.runner import SequentialRunner\n",
    "\n",
    "from purchase_predict.pipelines.processing.pipeline import create_pipeline\n",
    "\n",
    "def test_pipeline(catalog_test):\n",
    "    runner = SequentialRunner()\n",
    "    pipeline = create_pipeline()\n",
    "    pipeline_output = runner.run(pipeline, catalog_test)\n",
    "    assert pipeline_output[\"X_train\"].shape[0] == pipeline_output[\"y_train\"].shape[0]\n",
    "    assert pipeline_output[\"X_test\"].shape[0] == pipeline_output[\"y_test\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex√©cutons l'int√©gralit√© des tests."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "bash"
   },
   "source": [
    "kedro test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "========================= test session starts =========================\n",
    "platform linux -- Python 3.8.5, pytest-6.1.2, py-1.10.0, pluggy-0.13.1\n",
    "rootdir: /home/jovyan/purchase_predict, configfile: pyproject.toml\n",
    "plugins: mock-1.13.0, cov-2.11.0\n",
    "collected 6 items\n",
    "\n",
    "src/tests/test_run.py .                               [ 16%]\n",
    "src/tests/pipelines/loading/test_nodes.py .           [ 33%]\n",
    "src/tests/pipelines/loading/test_pipeline.py .        [ 50%]\n",
    "src/tests/pipelines/processing/test_nodes.py ..       [ 83%]\n",
    "src/tests/pipelines/processing/test_pipeline.py .     [100%]\n",
    "\n",
    "========================== 6 passed in 3.34s =========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voil√† ! Jusqu'ici, nos deux pipelines `loading` et `processing` sont test√©s. Nous avons r√©alis√© les tests pr√©-entra√Ænement. Mais qu'en est-il de ceux concernant le mod√®le ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "break": "new"
   },
   "source": [
    "## Tests post-entra√Ænement\n",
    "\n",
    "Les **tests post-entra√Ænement** vont √™tre ex√©cut√©s une fois le mod√®le calibr√©. Contrairement aux tests pr√©c√©dents, ils sont plus subtils car l'objectif est de mettre en √©vidence certains aspects et comportements particuliers du mod√®le pour v√©rifier qu'ils n'aboutissent pas √† des erreurs ou √† des incoh√©rences qui pourraient avoir d'importantes r√©percussions.\n",
    "\n",
    "Pour cela, nous pouvons faire intervenir plusieurs outils.\n",
    "\n",
    "- Des exemples de donn√©es dont on conna√Æt (en dehors de la base d'apprentissage) les r√©ponses.\n",
    "- Les m√©thodes d'interpr√©tabilit√© avec SHAP par exemple.\n",
    "- Des m√©thodes d'√©valuation de propri√©t√©s, comme la r√©gularit√© de la loi jointe.\n",
    "\n",
    "Commen√ßons par r√©cup√©rer le mod√®le de boosting que nous avions entra√Æn√© avec la base de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8539d7428c515feb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "model = joblib.load(os.path.expanduser(\"~/data/model.pkl\"))\n",
    "X_test = pd.read_csv(os.path.expanduser(\"~/data/X_test.csv\"))\n",
    "y_test = pd.read_csv(os.path.expanduser(\"~/data/y_test.csv\")).values.flatten()\n",
    "\n",
    "# On calcul ici les valeurs de Shapley\n",
    "explainer = shap.TreeExplainer(model)\n",
    "X_shap = X_test.copy()\n",
    "shap_values = explainer.shap_values(X_shap)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests d'invariance\n",
    "\n",
    "Les tests d'invariance nous permettent de d√©finir un ensemble de perturbations √† appliquer sur une ou plusieurs observations pour observer √† quel point cela affecte la sortie du mod√®le.\n",
    "\n",
    "Par exemple, supposons qu'un utilisateur, ayant visit√© un produit dont le prix est de 59‚Ç¨. Un test d'invariance consisterai √† dire qu'une variation de $\\pm 1‚Ç¨$ ne devrait pas faire varier la probabilit√© d'acheter de $\\pm x\\%$. Une variation importante pourrait signifier que cette variable a beaucoup d'impact, alors qu'en r√©alit√©, il est peu probable que pour un article de 59‚Ç¨ une variation de 1‚Ç¨ fasse drastiquement augmenter ou baisser la probabilit√©.\n",
    "\n",
    "S√©lectionnons une seule observation al√©atoire `x_unit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f6c405aeba855f8"
   },
   "outputs": [],
   "source": [
    "x_unit = X_test.loc[4720, :].copy()\n",
    "\n",
    "print(\"Pos : {:2.3f}%\".format(model.predict_proba([x_unit])[0, 1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La probabilit√© associ√©e ici est d'environ 92%. Appliquons une perturbation de +1‚Ç¨ et -1‚Ç¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "294ed468ef130a28"
   },
   "outputs": [],
   "source": [
    "x_unit = X_test.loc[4720, :].copy()\n",
    "x_unit['price'] += 1\n",
    "print(\"Pos : {:2.3f}%\".format(model.predict_proba([x_unit])[0, 1] * 100))\n",
    "\n",
    "x_unit = X_test.loc[4720, :].copy()\n",
    "x_unit['price'] -= 1\n",
    "print(\"Pos : {:2.3f}%\".format(model.predict_proba([x_unit])[0, 1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur cette observation en particulier, la diff√©rence de probabilit√© est de $0.165\\%$. L'impact est **tr√®s limit√©**, indiquant que le mod√®le est r√©gulier au voisinage de ce point.\n",
    "\n",
    "Calculons maintenant cette diff√©rence (en valeur absolue) de probabilit√© (pour la classe positive) pour chaque observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41e7ade55e4c7f9c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# On ne s√©lectionne que les articles dont le prix est > √† 1‚Ç¨, sinon on aurait un prix ... n√©gatif !\n",
    "X_test_price = X_test[X_test['price'] > 1]\n",
    "X_test_price_plus = X_test_price.copy()\n",
    "X_test_price_plus['price'] += 1\n",
    "X_test_price_minus = X_test_price.copy()\n",
    "X_test_price_minus['price'] -= 1\n",
    "\n",
    "y_price = pd.DataFrame()\n",
    "y_price[\"y\"] = model.predict_proba(X_test_price)[:, 1]\n",
    "y_price[\"y+\"] = model.predict_proba(X_test_price_plus)[:, 1]\n",
    "y_price[\"y-\"] = model.predict_proba(X_test_price_minus)[:, 1]\n",
    "y_price[\"abs_delta\"] = np.abs(y_price[\"y-\"] - y_price[\"y+\"])\n",
    "y_price.sort_values(\"abs_delta\", ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons voir que pour une dizaine d'observations au moins, cette variable de 1‚Ç¨ contribue √† un diff√©rentiel de 20% √† 30% sur la probabilit√© pr√©dite (ce qui est tout de m√™me √©lev√©).\n",
    "\n",
    "Regardons de quelles observations il s'agit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bec79aa916a253d9"
   },
   "outputs": [],
   "source": [
    "idxs = list(y_price.sort_values(\"abs_delta\", ascending=False).head(n=100).index)\n",
    "X_test_price.loc[idxs, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous aurions pu penser qu'il s'agit d'articles dont le prix est faible. Et pourtant, l'ordre de grandeur est de 20 √† 50‚Ç¨ pour la plupart des articles. Regardons les valeurs de Shapley de ces m√™mes observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9344ff9030b03b01"
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[idxs, :], X_shap.loc[idxs, :], plot_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au global, l'impact est limit√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ee676e98464b142"
   },
   "outputs": [],
   "source": [
    "print(\"√âcart-type : {:2.2f}%\".format(y_price[\"abs_delta\"].std()))\n",
    "print(\"Proportion : {:2.2f}%\".format(\n",
    "    y_price[y_price[\"abs_delta\"] < 0.05].shape[0] / y_price.shape[0] * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En tra√ßant ce delta pour chaque observation dans l'ordre d√©croissant, nous pouvons voir appara√Ætre un ¬´ coude ¬ª √† partir duquel ce delta stagne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14f7f9298cb22ff0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_obs = 1000\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.plot(\n",
    "    range(n_obs),\n",
    "    y_price.sort_values(\"abs_delta\", ascending=False).iloc[:n_obs, -1],\n",
    "    lw=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre situation, une variation de 10% **para√Æt raisonnable**. Il serait donc int√©ressant de se concentrer sur les quelques observations qui pr√©sentent une variation de plus de 20% par rapport √† la variable prix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test directionnels\n",
    "\n",
    "Les tests directionnels semblent proches des tests d'invariance, √† la diff√©rence pr√®s que l'ensemble des perturbations que nous allons appliquer aux observations devraient avoir un effet **connu √† l'avance** sur la sortie du mod√®le.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e1588fb83856300"
   },
   "outputs": [],
   "source": [
    "x_unit = X_test.loc[375, :]\n",
    "\n",
    "model.predict_proba([x_unit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'interpr√©tation locale est d'une grande aide : pourquoi y a-t-il une forte probabilit√© que cet utilisateur ne finalise pas l'achat ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed0612aebae64df9"
   },
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0], shap_values[375, :], x_unit, matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est principalement la dur√©e, qui n'est que de 10 secondes, qui explique pourquoi cet utilisateur ne finaliserai pas l'achat.\n",
    "\n",
    "Le but du test directionnel est de se poser la question suivante : et si la dur√©e avait dur√©e 60 secondes de plus, que se passerait-il ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "824281afd306024c"
   },
   "outputs": [],
   "source": [
    "x_unit = X_test.iloc[375, :].copy()\n",
    "x_unit['duration'] += 60\n",
    "\n",
    "model.predict_proba([x_unit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, la probabilit√© augmente de pr√®s de 70%, alors que la variable n'a augment√© que de 60 secondes. Ce qu'il faut regarder ici, ce sont les autres variables de l'observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "857c3ffe96cf4a94"
   },
   "outputs": [],
   "source": [
    "X_test.iloc[375, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons le graphe de d√©pendance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3086aee93513bb34"
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"duration\", shap_values, X_shap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici , l'interaction avec la variable `num_views_session` **est tr√®s forte** lorsque la dur√©e est tr√®s basse. Autrement dit, de petites dur√©es font fortement baisser la probabilit√© lorsqu'il n'y a que peu de vues dans une session.\n",
    "\n",
    "Maintenant, essayons conjointement d'augmenter la valeur de la variable `num_views_session`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "867564f6fdf45e23"
   },
   "outputs": [],
   "source": [
    "x_unit = X_test.iloc[375, :].copy()\n",
    "x_unit['duration'] += 10\n",
    "x_unit['num_views_session'] += 10\n",
    "\n",
    "model.predict_proba([x_unit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce contexte, la probabilit√© **reste tr√®s faible**. Ce test directionnel s'int√©resserait donc √† des observations avec de faibles dur√©es et peu de vues.\n",
    "\n",
    "Prenons un autre exemple, cette fois-ci pour un utilisateur ayant une forte probabilit√© de finaliser son achat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7696374c118761c6"
   },
   "outputs": [],
   "source": [
    "x_unit = X_test.loc[4720, :]\n",
    " \n",
    "model.predict_proba([x_unit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21204d7071a36687"
   },
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0], shap_values[4720, :], x_unit, matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retirons maintenant 60 secondes √† cette observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e16c7c325a19610f"
   },
   "outputs": [],
   "source": [
    "x_unit = X_test.loc[4720, :].copy()\n",
    "x_unit['duration'] -= 60\n",
    "    \n",
    "model.predict_proba([x_unit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'effet de la variable `duration` a beaucoup moins d'impact que pour l'observation pr√©c√©dente.\n",
    "\n",
    "Ce qu'il faut retenir, c'est qu'il ne suffit pas de d√©finir un seuil limite d'√©cart de probabilit√© en appliquant une perturbation $\\varepsilon$ sans √©tudier au pr√©alable l'observation qui va subir la transformation. Dans le premier exemple, la dur√©e √©tait tr√®s faible (seule 10 secondes), il √©tait donc logique sur la probabilit√© de finaliser l'achat soit tr√®s faible. En revanche, le fait de rajouter 60 secondes pour cette session peut cr√©er une observation que est pas ou tr√®s peu repr√©sent√©e dans l'√©chantillon : le mod√®le n'a rencontr√© que peu d'observations pr√©sentant ces caract√©ristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests unitaires du mod√®le\n",
    "\n",
    "Au m√™me titre que les tests unitaires sont r√©alis√©s pour les fonctions de collecte et de transformation de donn√©es, les tests unitaires pour le mod√®le consistent √† v√©rifier que ce dernier pr√©dit la bonne r√©ponse pour des observations qui sont suppos√©es √™tre parfaitement classifi√©es.\n",
    "\n",
    "Une m√©thode consiste √† calculer des **prototypes** : il s'agit d'observations qui *repr√©sentent le plus* les donn√©es. En d'autres termes, il s'agit d'un concept proches des centres de clusters form√©s par les observations. Et un algorithme non-supervis√© permettant de d√©tecter les prototypes est le **k-m√©do√Øde**, proche des k-moyennes dans son fonctionnement mais qui calcule le <a href=\"https://en.wikipedia.org/wiki/Medoid\" target=\"_blank\">m√©do√Øde</a>, point d'un cluster dont la distance avec tous les autres points est la plus petite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "206b5b2c0f9ffd31"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn-extra -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lan√ßons un k-m√©do√Øde sur les observations de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39c38396c8de591d"
   },
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "kmed = KMedoids(n_clusters=10)\n",
    "kmed.fit(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R√©cup√©rons les centres des clusters (les m√©do√Ødes) dans un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0adba32d7d7533c0"
   },
   "outputs": [],
   "source": [
    "X_prototypes = pd.DataFrame(\n",
    "    data=kmed.cluster_centers_,\n",
    "    columns=X_test.columns\n",
    ")\n",
    "X_prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chacune de ces observations repr√©sentent la moyenne d'une sous-population de l'√©chantillon. √âtonnamment, hormis la premi√®re observation, toutes les autres concernent des produits issus de la m√™me cat√©gorie.\n",
    "\n",
    "Calculons les probabilit√© associ√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e427d357fcdf81cd"
   },
   "outputs": [],
   "source": [
    "model.predict_proba(kmed.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L√†-aussi, √† part la premi√®re observation, toutes les autres sont pr√©dites dans la classe positive. La derni√®re observation est, quant-√†-elle, plus difficile √† quantifier du fait des deux probabilit√©s tr√®s proches.\n",
    "\n",
    "Nous pourrions ainsi extraire plusieurs prototypes de ce DataFrame. Attention n√©anmoins, car ces donn√©es repr√©sentent uniquement un historique d'une journ√©e, alors qu'en pratique, celles qui seront utilis√©es pour calibrer le mod√®le repr√©sentent un historique de 7 jours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f57b2dda03f3ad1"
   },
   "source": [
    "## ‚úîÔ∏è Conclusion\n",
    "\n",
    "Les tests de mod√®le sont plus difficiles √† construire, mais sont indispensables pour certains secteurs d'activit√©s o√π les pr√©dictions du mod√®le peuvent √™tre critiques.\n",
    "\n",
    "- Nous avons vu les test pr√©-entra√Ænement pour s'assurer de la coh√©rence de la base d'apprentissage avant l'entra√Ænement.\n",
    "- Nous avons d√©taill√© plusieurs tests de mod√®les pour v√©rifier son comportement.\n",
    "\n",
    "> ‚û°Ô∏è Lorsque ces tests sont r√©alis√©s avec succ√®s, il faut maintenant conserver le mod√®le quelque part pour y acc√©der ult√©rieurement : c'est le r√¥le de <b>MLflow</b>."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "√âditer les M√©ta-Donn√©es",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
